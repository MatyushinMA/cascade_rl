{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from scipy.special import expit as sigmoid\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import time\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from a2c_ppo_acktr import utils\n",
    "import random\n",
    "\n",
    "from heap import Heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    bandwidth = 3\n",
    "    postwidth = 3\n",
    "    threshold = 0.1\n",
    "    slip_reward = -0.001\n",
    "    signal_split = 1\n",
    "    fps = 3\n",
    "    log_interval = 250\n",
    "    use_gae = False\n",
    "    num_updates = 1e5\n",
    "    num_steps = 32\n",
    "    sleep_freq = 0\n",
    "    sleep_skip = False\n",
    "    memory_capacity = num_steps*fps*20\n",
    "    ppo_epoch = 1\n",
    "    num_mini_batch = 32\n",
    "    value_loss_coef = 0.5\n",
    "    entropy_coef = 0.01\n",
    "    lr = 1e-3\n",
    "    eps = 1e-5\n",
    "    max_grad_norm = 0.2\n",
    "    clip_param = 0.2\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    use_proper_time_limits = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f1ery/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "heap = Heap(13, 4, 1, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f1ery/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 250, Avg/Max/Min. reward: 14.7/55.0/8.0, Avg relaxation: 0.23\n",
      "Iter: 500, Avg/Max/Min. reward: 9.8/21.0/8.0, Avg relaxation: 0.78\n",
      "Iter: 750, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 1000, Avg/Max/Min. reward: 8.9/14.0/8.0, Avg relaxation: 0.96\n",
      "Iter: 1250, Avg/Max/Min. reward: 10.5/64.0/8.0, Avg relaxation: 0.74\n",
      "Iter: 1500, Avg/Max/Min. reward: 11.9/39.0/8.0, Avg relaxation: 0.57\n",
      "Iter: 1750, Avg/Max/Min. reward: 9.4/22.0/8.0, Avg relaxation: 0.86\n",
      "Iter: 2000, Avg/Max/Min. reward: 9.4/16.0/8.0, Avg relaxation: 0.88\n",
      "Iter: 2250, Avg/Max/Min. reward: 9.4/21.0/8.0, Avg relaxation: 0.88\n",
      "Iter: 2500, Avg/Max/Min. reward: 9.2/14.0/8.0, Avg relaxation: 0.90\n",
      "Iter: 2750, Avg/Max/Min. reward: 9.1/12.0/8.0, Avg relaxation: 0.92\n",
      "Iter: 3000, Avg/Max/Min. reward: 9.0/13.0/8.0, Avg relaxation: 0.94\n",
      "Iter: 3250, Avg/Max/Min. reward: 9.0/14.0/8.0, Avg relaxation: 0.96\n",
      "Iter: 3500, Avg/Max/Min. reward: 8.9/15.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 3750, Avg/Max/Min. reward: 9.0/13.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 4000, Avg/Max/Min. reward: 8.9/11.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 4250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 4500, Avg/Max/Min. reward: 9.0/12.0/8.0, Avg relaxation: 0.96\n",
      "Iter: 4750, Avg/Max/Min. reward: 8.9/11.0/8.0, Avg relaxation: 0.96\n",
      "Iter: 5000, Avg/Max/Min. reward: 9.0/15.0/8.0, Avg relaxation: 0.95\n",
      "Iter: 5250, Avg/Max/Min. reward: 9.0/11.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 5500, Avg/Max/Min. reward: 8.9/13.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 5750, Avg/Max/Min. reward: 8.9/12.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 6000, Avg/Max/Min. reward: 8.9/13.0/8.0, Avg relaxation: 0.96\n",
      "Iter: 6250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 0.97\n",
      "Iter: 6500, Avg/Max/Min. reward: 8.9/13.0/8.0, Avg relaxation: 0.96\n",
      "Iter: 6750, Avg/Max/Min. reward: 8.9/11.0/8.0, Avg relaxation: 0.98\n",
      "Iter: 7000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 0.99\n",
      "Iter: 7250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 0.99\n",
      "Iter: 7500, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 7750, Avg/Max/Min. reward: 8.7/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 8000, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 8250, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 8500, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 8750, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 9000, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 9250, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 9500, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 9750, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 10000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 10250, Avg/Max/Min. reward: 8.7/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 10500, Avg/Max/Min. reward: 8.8/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 10750, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 11000, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 11250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 11500, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 11750, Avg/Max/Min. reward: 8.7/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 12000, Avg/Max/Min. reward: 8.8/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 12250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 12500, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 12750, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 13000, Avg/Max/Min. reward: 8.8/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 13250, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 13500, Avg/Max/Min. reward: 8.7/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 13750, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 14000, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 14250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 14500, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 14750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 15000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 15250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 15500, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 15750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 16000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 16250, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 16500, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 16750, Avg/Max/Min. reward: 8.7/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 17000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 17250, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 17500, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 17750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 18000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 18250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 18500, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 18750, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 19000, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 19250, Avg/Max/Min. reward: 8.8/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 19500, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 19750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 20000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 20250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 20500, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 20750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 21000, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 21250, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 21500, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 21750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 22000, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 22250, Avg/Max/Min. reward: 8.8/10.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 22500, Avg/Max/Min. reward: 8.7/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 22750, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 23000, Avg/Max/Min. reward: 8.7/9.0/8.0, Avg relaxation: 1.00\n",
      "Iter: 23250, Avg/Max/Min. reward: 8.8/11.0/8.0, Avg relaxation: 1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-46784c8ce42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeros_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mepisode_relaxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter: %d, Avg/Max/Min. reward: %0.1f/%0.1f/%0.1f, Avg relaxation: %0.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_relaxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/on_policy/heap.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/on_policy/unit.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                self.args.gae_lambda, self.args.use_proper_time_limits)\n\u001b[1;32m     69\u001b[0m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_entropies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/on_policy/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_loss_coef\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_coef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = deque(maxlen=args.log_interval)\n",
    "episode_relaxes = deque(maxlen=args.log_interval)\n",
    "done = True\n",
    "\n",
    "for j in count(len(heap.units[0].action_losses) + 1):\n",
    "    heap.clear_memory()\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_rewards.append(0)\n",
    "        episode_relaxes.append(0)\n",
    "        heap.reset()\n",
    "    for unit in heap.units:\n",
    "        utils.update_linear_schedule(unit.agent.optimizer, j, args.num_updates, args.lr)\n",
    "    for step in range(args.num_steps):\n",
    "        if args.sleep_freq and episode_rewards[-1] % args.sleep_freq == 0 and episode_rewards[-1]:\n",
    "            heap.sleep(skip=args.sleep_skip)\n",
    "        action, zeros_mask = heap(state, n=args.fps)\n",
    "        action = int(action > 0.5)\n",
    "        if len(zeros_mask[0]):\n",
    "            if state[2] > 0:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        heap.reward(reward)\n",
    "        episode_rewards[-1] += reward\n",
    "        if done:\n",
    "            heap.done()\n",
    "        if len(zeros_mask[0]) and reward:\n",
    "            episode_relaxes[-1] += 1\n",
    "    heap.update()\n",
    "    if j % args.log_interval == 0:\n",
    "        print('Iter: %d, Avg/Max/Min. reward: %0.1f/%0.1f/%0.1f, Avg relaxation: %0.2f' % (j, sum(episode_rewards)/len(episode_rewards), max(episode_rewards), min(episode_rewards), sum(episode_relaxes)/sum(episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "def moving_average(a, k=n) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[k:] = ret[k:] - ret[:-k]\n",
    "    return ret[k - 1:] / k\n",
    "print('INPUTS')\n",
    "for j in range(heap.num_inputs):\n",
    "    label = 'Unit %d' % (j + 1)\n",
    "    plt.plot(list(range(len(heap.weights)))[:1-10*n], moving_average(list(map(lambda x : x[j], heap.weights)), 10*n), label=label)\n",
    "plt.title('Weight')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('HIDDEN')\n",
    "for j in range(heap.num_units - heap.num_inputs - heap.num_outputs):\n",
    "    label = 'Unit %d' % (j + 1)\n",
    "    plt.plot(list(range(len(heap.weights)))[:1-10*n], moving_average(list(map(lambda x : x[heap.num_inputs + j], heap.weights)), 10*n), label=label)\n",
    "plt.title('Weight')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('OUTPUTS')\n",
    "for j in range(heap.num_outputs):\n",
    "    label = 'Unit %d' % (j + 1)\n",
    "    plt.plot(list(range(len(heap.weights)))[:1-10*n], moving_average(list(map(lambda x : x[heap.num_units - j - 1], heap.weights)), 10*n), label=label)\n",
    "plt.title('Weight')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "heap.plot_stats(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
