{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from scipy.special import expit as sigmoid\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import time\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from a2c_ppo_acktr import utils\n",
    "import random\n",
    "\n",
    "from heap import Heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    bandwidth = 3\n",
    "    postwidth = 3\n",
    "    threshold = 0.1\n",
    "    slip_reward = -0.001\n",
    "    signal_split = 1\n",
    "    fps = 3\n",
    "    log_interval = 1\n",
    "    use_gae = False\n",
    "    num_updates = 1e5\n",
    "    num_steps = 10\n",
    "    sleep_freq = 0\n",
    "    sleep_skip = False\n",
    "    memory_capacity = num_steps*fps*13\n",
    "    ppo_epoch = 1\n",
    "    num_mini_batch = 10\n",
    "    value_loss_coef = 0.5\n",
    "    entropy_coef = 0.01\n",
    "    lr = 1e-2\n",
    "    lbfgs_history = 1000\n",
    "    max_grad_norm = 0.005\n",
    "    clip_param = 0.005\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    use_proper_time_limits = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "heap = Heap(13, 4, 1, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f1ery/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2, Avg/Max/Min. reward: 16.0/16.0/16.0, Avg relaxation: 0.31\n",
      "Iter: 3, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 1.00\n",
      "Iter: 4, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 1.00\n",
      "Iter: 5, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 1.00\n",
      "Iter: 6, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.90\n",
      "Iter: 7, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.90\n",
      "Iter: 8, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 0.89\n",
      "Iter: 9, Avg/Max/Min. reward: 8.0/8.0/8.0, Avg relaxation: 0.62\n",
      "Iter: 10, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.50\n",
      "Iter: 11, Avg/Max/Min. reward: 14.0/14.0/14.0, Avg relaxation: 0.57\n",
      "Iter: 12, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.60\n",
      "Iter: 13, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.70\n",
      "Iter: 14, Avg/Max/Min. reward: 16.0/16.0/16.0, Avg relaxation: 0.75\n",
      "Iter: 15, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.60\n",
      "Iter: 16, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 0.78\n",
      "Iter: 17, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.80\n",
      "Iter: 18, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 0.89\n",
      "Iter: 19, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 0.78\n",
      "Iter: 20, Avg/Max/Min. reward: 8.0/8.0/8.0, Avg relaxation: 0.75\n",
      "Iter: 21, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.90\n",
      "Iter: 22, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.70\n",
      "Iter: 23, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.80\n",
      "Iter: 24, Avg/Max/Min. reward: 11.0/11.0/11.0, Avg relaxation: 0.73\n",
      "Iter: 25, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 0.67\n",
      "Iter: 26, Avg/Max/Min. reward: 9.0/9.0/9.0, Avg relaxation: 0.56\n",
      "Iter: 27, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.60\n",
      "Iter: 28, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.80\n",
      "Iter: 29, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.70\n",
      "Iter: 30, Avg/Max/Min. reward: 12.0/12.0/12.0, Avg relaxation: 0.67\n",
      "Iter: 31, Avg/Max/Min. reward: 8.0/8.0/8.0, Avg relaxation: 0.88\n",
      "Iter: 32, Avg/Max/Min. reward: 10.0/10.0/10.0, Avg relaxation: 0.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-46784c8ce42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeros_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mepisode_relaxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter: %d, Avg/Max/Min. reward: %0.1f/%0.1f/%0.1f, Avg relaxation: %0.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_relaxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/on_policy/heap.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/on_policy/unit.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m         self.memory.compute_returns(next_value, self.args.use_gae, self.args.gamma,\n\u001b[1;32m     69\u001b[0m                                self.args.gae_lambda, self.args.use_proper_time_limits)\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/on_policy/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     88\u001b[0m                                          \u001b[0;31m# self.max_grad_norm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mvalue_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0maction_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_diag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mbe_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbe_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = deque(maxlen=args.log_interval)\n",
    "episode_relaxes = deque(maxlen=args.log_interval)\n",
    "done = True\n",
    "\n",
    "for j in count(len(heap.units[0].action_losses) + 1):\n",
    "    heap.clear_memory()\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_rewards.append(0)\n",
    "        episode_relaxes.append(0)\n",
    "        heap.reset()\n",
    "    for unit in heap.units:\n",
    "        utils.update_linear_schedule(unit.agent.optimizer, j, args.num_updates, args.lr)\n",
    "    for step in range(args.num_steps):\n",
    "        if args.sleep_freq and episode_rewards[-1] % args.sleep_freq == 0 and episode_rewards[-1]:\n",
    "            heap.sleep(skip=args.sleep_skip)\n",
    "        action, zeros_mask = heap(state, n=args.fps)\n",
    "        action = int(action > 0.5)\n",
    "        if len(zeros_mask[0]):\n",
    "            if state[2] > 0:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        heap.reward(reward)\n",
    "        episode_rewards[-1] += reward\n",
    "        if done:\n",
    "            heap.done()\n",
    "        if len(zeros_mask[0]) and reward:\n",
    "            episode_relaxes[-1] += 1\n",
    "    heap.update()\n",
    "    if j % args.log_interval == 0:\n",
    "        print('Iter: %d, Avg/Max/Min. reward: %0.1f/%0.1f/%0.1f, Avg relaxation: %0.2f' % (j, sum(episode_rewards)/len(episode_rewards), max(episode_rewards), min(episode_rewards), sum(episode_relaxes)/sum(episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heap.plot_stats(n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
