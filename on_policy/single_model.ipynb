{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.algo import PPO\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr import utils\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape = (1,)\n",
    "env.action_space.high = np.array([1])\n",
    "env.action_space.low = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = Policy(\n",
    "        env.observation_space.shape,\n",
    "        env.action_space,\n",
    "        base_kwargs={'recurrent': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    eval_interval = None\n",
    "    log_interval = 10\n",
    "    use_gae = False\n",
    "    num_env_steps = 10e6\n",
    "    num_steps = 32\n",
    "    clip_param = 0.2\n",
    "    ppo_epoch = 4\n",
    "    num_mini_batch = 32\n",
    "    value_loss_coef = 0.5\n",
    "    entropy_coef = 0.01\n",
    "    lr = 7e-4\n",
    "    eps = 1e-5\n",
    "    max_grad_norm = 0.5\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    use_proper_time_limits = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = RolloutStorage(args.num_steps, 1,\n",
    "                        env.observation_space.shape, env.action_space,\n",
    "                        actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 32 \n",
      " Last 6 training episodes: mean/median reward 27.0/27.0, min/max reward 27.0/27.0\n",
      "\n",
      "Updates 10, num timesteps 352 \n",
      " Last 10 training episodes: mean/median reward 38.0/38.0, min/max reward 38.0/38.0\n",
      "\n",
      "Updates 20, num timesteps 672 \n",
      " Last 10 training episodes: mean/median reward 18.0/18.0, min/max reward 18.0/18.0\n",
      "\n",
      "Updates 30, num timesteps 992 \n",
      " Last 10 training episodes: mean/median reward 24.0/24.0, min/max reward 24.0/24.0\n",
      "\n",
      "Updates 40, num timesteps 1312 \n",
      " Last 10 training episodes: mean/median reward 21.0/21.0, min/max reward 21.0/21.0\n",
      "\n",
      "Updates 50, num timesteps 1632 \n",
      " Last 10 training episodes: mean/median reward 27.6/29.0, min/max reward 19.0/32.0\n",
      "\n",
      "Updates 60, num timesteps 1952 \n",
      " Last 10 training episodes: mean/median reward 52.0/52.0, min/max reward 52.0/52.0\n",
      "\n",
      "Updates 70, num timesteps 2272 \n",
      " Last 10 training episodes: mean/median reward 74.0/74.0, min/max reward 74.0/74.0\n",
      "\n",
      "Updates 80, num timesteps 2592 \n",
      " Last 10 training episodes: mean/median reward 23.4/27.0, min/max reward 18.0/27.0\n",
      "\n",
      "Updates 90, num timesteps 2912 \n",
      " Last 10 training episodes: mean/median reward 45.0/45.0, min/max reward 45.0/45.0\n",
      "\n",
      "Updates 100, num timesteps 3232 \n",
      " Last 10 training episodes: mean/median reward 54.0/54.0, min/max reward 54.0/54.0\n",
      "\n",
      "Updates 110, num timesteps 3552 \n",
      " Last 10 training episodes: mean/median reward 35.0/35.0, min/max reward 35.0/35.0\n",
      "\n",
      "Updates 120, num timesteps 3872 \n",
      " Last 10 training episodes: mean/median reward 42.8/44.0, min/max reward 32.0/44.0\n",
      "\n",
      "Updates 130, num timesteps 4192 \n",
      " Last 10 training episodes: mean/median reward 76.0/76.0, min/max reward 76.0/76.0\n",
      "\n",
      "Updates 140, num timesteps 4512 \n",
      " Last 10 training episodes: mean/median reward 137.0/137.0, min/max reward 124.0/150.0\n",
      "\n",
      "Updates 150, num timesteps 4832 \n",
      " Last 10 training episodes: mean/median reward 148.0/148.0, min/max reward 148.0/148.0\n",
      "\n",
      "Updates 160, num timesteps 5152 \n",
      " Last 10 training episodes: mean/median reward 92.2/92.5, min/max reward 58.0/124.0\n",
      "\n",
      "Updates 170, num timesteps 5472 \n",
      " Last 10 training episodes: mean/median reward 140.0/140.0, min/max reward 140.0/140.0\n",
      "\n",
      "Updates 180, num timesteps 5792 \n",
      " Last 10 training episodes: mean/median reward 168.0/168.0, min/max reward 168.0/168.0\n",
      "\n",
      "Updates 190, num timesteps 6112 \n",
      " Last 10 training episodes: mean/median reward 149.0/149.0, min/max reward 149.0/149.0\n",
      "\n",
      "Updates 200, num timesteps 6432 \n",
      " Last 10 training episodes: mean/median reward 89.0/89.0, min/max reward 89.0/89.0\n",
      "\n",
      "Updates 210, num timesteps 6752 \n",
      " Last 10 training episodes: mean/median reward 169.0/169.0, min/max reward 169.0/169.0\n",
      "\n",
      "Updates 220, num timesteps 7072 \n",
      " Last 10 training episodes: mean/median reward 217.2/219.5, min/max reward 192.0/224.0\n",
      "\n",
      "Updates 230, num timesteps 7392 \n",
      " Last 10 training episodes: mean/median reward 219.5/219.5, min/max reward 215.0/224.0\n",
      "\n",
      "Updates 240, num timesteps 7712 \n",
      " Last 10 training episodes: mean/median reward 101.0/101.0, min/max reward 101.0/101.0\n",
      "\n",
      "Updates 250, num timesteps 8032 \n",
      " Last 10 training episodes: mean/median reward 85.0/85.0, min/max reward 85.0/85.0\n",
      "\n",
      "Updates 260, num timesteps 8352 \n",
      " Last 10 training episodes: mean/median reward 68.5/58.0, min/max reward 58.0/93.0\n",
      "\n",
      "Updates 270, num timesteps 8672 \n",
      " Last 10 training episodes: mean/median reward 71.0/71.0, min/max reward 71.0/71.0\n",
      "\n",
      "Updates 280, num timesteps 8992 \n",
      " Last 10 training episodes: mean/median reward 17.0/17.0, min/max reward 17.0/17.0\n",
      "\n",
      "Updates 290, num timesteps 9312 \n",
      " Last 10 training episodes: mean/median reward 22.1/26.0, min/max reward 13.0/26.0\n",
      "\n",
      "Updates 300, num timesteps 9632 \n",
      " Last 10 training episodes: mean/median reward 14.0/14.0, min/max reward 14.0/14.0\n",
      "\n",
      "Updates 310, num timesteps 9952 \n",
      " Last 10 training episodes: mean/median reward 16.0/10.0, min/max reward 10.0/30.0\n",
      "\n",
      "Updates 320, num timesteps 10272 \n",
      " Last 10 training episodes: mean/median reward 47.0/47.0, min/max reward 47.0/47.0\n",
      "\n",
      "Updates 330, num timesteps 10592 \n",
      " Last 10 training episodes: mean/median reward 45.0/45.0, min/max reward 45.0/45.0\n",
      "\n",
      "Updates 340, num timesteps 10912 \n",
      " Last 10 training episodes: mean/median reward 21.0/21.0, min/max reward 21.0/21.0\n",
      "\n",
      "Updates 350, num timesteps 11232 \n",
      " Last 10 training episodes: mean/median reward 78.0/78.0, min/max reward 78.0/78.0\n",
      "\n",
      "Updates 360, num timesteps 11552 \n",
      " Last 10 training episodes: mean/median reward 52.0/52.0, min/max reward 52.0/52.0\n",
      "\n",
      "Updates 370, num timesteps 11872 \n",
      " Last 10 training episodes: mean/median reward 52.0/52.0, min/max reward 52.0/52.0\n",
      "\n",
      "Updates 380, num timesteps 12192 \n",
      " Last 10 training episodes: mean/median reward 17.0/17.0, min/max reward 17.0/17.0\n",
      "\n",
      "Updates 390, num timesteps 12512 \n",
      " Last 10 training episodes: mean/median reward 106.9/117.0, min/max reward 16.0/117.0\n",
      "\n",
      "Updates 400, num timesteps 12832 \n",
      " Last 10 training episodes: mean/median reward 149.0/149.0, min/max reward 149.0/149.0\n",
      "\n",
      "Updates 410, num timesteps 13152 \n",
      " Last 10 training episodes: mean/median reward 156.8/154.0, min/max reward 149.0/192.0\n",
      "\n",
      "Updates 420, num timesteps 13472 \n",
      " Last 10 training episodes: mean/median reward 219.4/219.5, min/max reward 215.0/223.0\n",
      "\n",
      "Updates 430, num timesteps 13792 \n",
      " Last 10 training episodes: mean/median reward 214.5/219.5, min/max reward 191.0/223.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b07b20078fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                            args.gae_lambda, args.use_proper_time_limits)\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/on_policy/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0msurr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madv_targ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n\u001b[0;32m---> 65\u001b[0;31m                                     1.0 + self.clip_param) * adv_targ\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0maction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "num_updates = int(args.num_env_steps) // args.num_steps\n",
    "\n",
    "done = True\n",
    "episode_reward = 0\n",
    "\n",
    "for j in range(num_updates):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        memory.obs[0].copy_(torch.from_numpy(state).float())\n",
    "        episode_reward = 0\n",
    "    \n",
    "    utils.update_linear_schedule(agent.optimizer, j, num_updates, args.lr)\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                memory.obs[step], memory.recurrent_hidden_states[step],\n",
    "                memory.masks[step])\n",
    "            actual_action = 0\n",
    "            if action > 0.5:\n",
    "                actual_action = 1\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        state, reward, done, info = env.step(actual_action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        mask = torch.FloatTensor([[1 - float(done)]])\n",
    "        bad_mask = torch.FloatTensor([[1 - float('bad_transition' in info.keys())]])\n",
    "        memory.insert(torch.from_numpy(state).float(), recurrent_hidden_states, action,\n",
    "                        action_log_prob, value, torch.FloatTensor([[reward]]), mask, bad_mask)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(\n",
    "            memory.obs[-1], memory.recurrent_hidden_states[-1],\n",
    "            memory.masks[-1]).detach()\n",
    "\n",
    "    memory.compute_returns(next_value, args.use_gae, args.gamma,\n",
    "                           args.gae_lambda, args.use_proper_time_limits)\n",
    "    \n",
    "    value_loss, action_loss, dist_entropy = agent.update(memory)\n",
    "    memory.after_update()\n",
    "    \n",
    "    if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "        total_num_steps = (j + 1) * args.num_steps\n",
    "        print(\n",
    "            \"Updates {}, num timesteps {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "            .format(j, total_num_steps,\n",
    "                    len(episode_rewards), np.mean(episode_rewards),\n",
    "                    np.median(episode_rewards), np.min(episode_rewards),\n",
    "                    np.max(episode_rewards), dist_entropy, value_loss,\n",
    "                    action_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196.0\n",
      "200.0\n",
      "200.0\n",
      "178.0\n",
      "200.0\n",
      "185.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "173.0\n",
      "198.0\n",
      "199.0\n",
      "200.0\n",
      "182.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "162.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "175.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f3e9dd10c8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n\u001b[0;32m----> 8\u001b[0;31m                 torch.from_numpy(state).float(), None, None)\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mactual_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/on_policy/a2c_ppo_acktr/model.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, inputs, rnn_hxs, masks, deterministic)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/on_policy/a2c_ppo_acktr/distributions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFixedCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    env.render()\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                torch.from_numpy(state).float(), None)\n",
    "            actual_action = 0\n",
    "            if action > 0.5:\n",
    "                actual_action = 1\n",
    "        state, reward, done, _ = env.step(actual_action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(episode_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
