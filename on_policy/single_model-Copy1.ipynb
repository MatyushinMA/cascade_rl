{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.algo import PPO, A2C_ACKTR\n",
    "from a2c_ppo_acktr.storage import RolloutStorage, ExtendableStorage\n",
    "from a2c_ppo_acktr import utils\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f1ery/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape = (1,)\n",
    "env.action_space.high = np.array([1])\n",
    "env.action_space.low = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = Policy(\n",
    "        (4,),\n",
    "        env.action_space,\n",
    "        base_kwargs={'recurrent': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    eval_interval = None\n",
    "    log_interval = 10\n",
    "    use_gae = False\n",
    "    num_env_steps = 10e6\n",
    "    num_steps = 32\n",
    "    ppo_epoch = 4\n",
    "    num_mini_batch = 32\n",
    "    memory_capacity = 32\n",
    "    value_loss_coef = 0.5\n",
    "    entropy_coef = 0.01\n",
    "    lr = 7e-5\n",
    "    eps = 1e-5\n",
    "    max_grad_norm = 0.05\n",
    "    clip_param = 0.05\n",
    "    alpha = 0.99\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    use_proper_time_limits = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C_ACKTR(\n",
    "        actor_critic,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        alpha=args.alpha,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        acktr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = RolloutStorage(args.num_steps, 1,\n",
    "                        (4,), env.action_space,\n",
    "                        actor_critic.recurrent_hidden_state_size)\n",
    "my_memory = ExtendableStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_rewards = deque(maxlen=10)\n",
    "value_losses = []\n",
    "action_losses = []\n",
    "dist_entropies = []\n",
    "\n",
    "num_updates = int(args.num_env_steps) // args.num_steps\n",
    "\n",
    "done = True\n",
    "episode_reward = 0\n",
    "\n",
    "for j in range(num_updates):\n",
    "    memory = RolloutStorage(args.num_steps, 1,\n",
    "                        (4,), env.action_space,\n",
    "                        actor_critic.recurrent_hidden_state_size)\n",
    "    my_memory.clear()\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        memory.obs[0].copy_(torch.from_numpy(state).float())\n",
    "        episode_reward = 0\n",
    "    \n",
    "    #utils.update_linear_schedule(agent.optimizer, j, num_updates, args.lr)\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                memory.obs[step], memory.recurrent_hidden_states[step],\n",
    "                memory.masks[step])\n",
    "            actual_action = int(action > 0.5)\n",
    "        \n",
    "        my_memory.insert(torch.from_numpy(state).float(), action, action_log_prob, value, unit_id=random.randint(0, 1))\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        state, reward, done, info = env.step(actual_action)\n",
    "        my_memory.reward(reward)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # If done then clean the history of observations.\n",
    "        mask = torch.FloatTensor([[1 - float(done)]])\n",
    "        bad_mask = torch.FloatTensor([[1 - float('bad_transition' in info.keys())]])\n",
    "        memory.insert(torch.from_numpy(state).float(), recurrent_hidden_states, action,\n",
    "                        action_log_prob, value, torch.FloatTensor([[reward]]), mask, bad_mask)\n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            break\n",
    "    break\n",
    "    \n",
    "    \"\"\"with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(\n",
    "            memory.obs[-1], memory.recurrent_hidden_states[-1],\n",
    "            memory.masks[-1]).detach()\n",
    "\n",
    "    memory.compute_returns(next_value, args.use_gae, args.gamma,\n",
    "                           args.gae_lambda, args.use_proper_time_limits)\n",
    "    \n",
    "    value_loss, action_loss, dist_entropy = agent.update(memory)\n",
    "    value_losses.append(value_loss)\n",
    "    action_losses.append(action_loss)\n",
    "    dist_entropies.append(dist_entropy)\n",
    "    memory.after_update()\n",
    "    \n",
    "    if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "        eval_rewards = []\n",
    "        done = True\n",
    "        for i in tqdm(range(250), desc='Eval'):\n",
    "            _done = False\n",
    "            state = env.reset()[:1]\n",
    "            eval_rewards.append(0)\n",
    "            while not _done:\n",
    "                _, action, _, _ = actor_critic.act(torch.from_numpy(state).float().view((1, 1)), None, None)\n",
    "                action = int(action > 0.5)\n",
    "                state, reward, _done, _ = env.step(action)\n",
    "                state = state[:1]\n",
    "                eval_rewards[-1] += reward\n",
    "        total_num_steps = (j + 1) * args.num_steps\n",
    "        print(\n",
    "            \"Updates {}, num timesteps {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\neval episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "            .format(j, total_num_steps,\n",
    "                    len(episode_rewards), np.mean(episode_rewards),\n",
    "                    np.median(episode_rewards), np.min(episode_rewards),\n",
    "                    np.max(episode_rewards), np.mean(eval_rewards),\n",
    "                    np.median(eval_rewards), np.min(eval_rewards),\n",
    "                    np.max(eval_rewards)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value = actor_critic.get_value(\n",
    "            torch.from_numpy(state).float(), None,\n",
    "            None).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.compute_returns(next_value, False, args.gamma, 0.95, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_memory.compute_returns(next_value, args.gamma, done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_memory.returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_memory.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_memory.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_memory.value_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_memory.action_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_advantages = (my_memory.returns[:-1] - my_memory.value_preds[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = my_memory.feed_forward_generator(my_advantages, unit_id=0, num_mini_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0147, -0.0495, -0.0177,  0.0128]),\n",
       " tensor([ 0.0137,  0.1458, -0.0174, -0.2854]),\n",
       " tensor([ 0.0167, -0.0490, -0.0231,  0.0018]),\n",
       " tensor([ 0.0157, -0.2438, -0.0231,  0.2871]),\n",
       " tensor([ 0.0108, -0.0484, -0.0173, -0.0128]),\n",
       " tensor([ 0.0098,  0.1470, -0.0176, -0.3109]),\n",
       " tensor([ 0.0128, -0.0479, -0.0238, -0.0238]),\n",
       " tensor([ 0.0118,  0.1476, -0.0243, -0.3239]),\n",
       " tensor([ 0.0148, -0.0472, -0.0308, -0.0390]),\n",
       " tensor([ 0.0138, -0.2418, -0.0315,  0.2438]),\n",
       " tensor([ 0.0090, -0.4365, -0.0267,  0.5264]),\n",
       " tensor([ 0.0003, -0.2410, -0.0161,  0.2254]),\n",
       " tensor([-0.0046, -0.0457, -0.0116, -0.0723]),\n",
       " tensor([-0.0055,  0.1496, -0.0131, -0.3686]),\n",
       " tensor([-0.0025,  0.3449, -0.0204, -0.6654]),\n",
       " tensor([ 0.0044,  0.1501, -0.0338, -0.3792]),\n",
       " tensor([ 0.0074, -0.0445, -0.0413, -0.0974]),\n",
       " tensor([ 0.0065, -0.2390, -0.0433,  0.1820]),\n",
       " tensor([ 0.0017, -0.4335, -0.0396,  0.4607]),\n",
       " tensor([-0.0069, -0.6281, -0.0304,  0.7406]),\n",
       " tensor([-0.0195, -0.4325, -0.0156,  0.4385]),\n",
       " tensor([-0.0281, -0.2372, -0.0068,  0.1410]),\n",
       " tensor([-0.0329, -0.4322, -0.0040,  0.4315]),\n",
       " tensor([-0.0415, -0.2370,  0.0046,  0.1375]),\n",
       " tensor([-0.0463, -0.0420,  0.0074, -0.1537]),\n",
       " tensor([-0.0471, -0.2372,  0.0043,  0.1413]),\n",
       " tensor([-0.0518, -0.4324,  0.0071,  0.4353]),\n",
       " tensor([-0.0605, -0.2374,  0.0158,  0.1449]),\n",
       " tensor([-0.0652, -0.4327,  0.0187,  0.4425]),\n",
       " tensor([-0.0739, -0.6281,  0.0276,  0.7411]),\n",
       " tensor([-0.0865, -0.8236,  0.0424,  1.0423]),\n",
       " tensor([-0.1029, -1.0192,  0.0632,  1.3480])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_memory.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1845e-02, -4.3239e-01,  7.1041e-03,  4.3534e-01],\n",
      "        [ 6.5296e-03, -2.3904e-01, -4.3287e-02,  1.8200e-01],\n",
      "        [-3.2877e-02, -4.3221e-01, -4.0290e-03,  4.3149e-01],\n",
      "        [ 1.1820e-02,  1.4759e-01, -2.4286e-02, -3.2390e-01],\n",
      "        [ 1.4772e-02, -4.7178e-02, -3.0764e-02, -3.8976e-02],\n",
      "        [ 1.2777e-02, -4.7865e-02, -2.3810e-02, -2.3803e-02],\n",
      "        [ 1.5681e-02, -2.4381e-01, -2.3078e-02,  2.8709e-01],\n",
      "        [-2.8133e-02, -2.3719e-01, -6.8485e-03,  1.4098e-01],\n",
      "        [ 1.4735e-02, -4.9526e-02, -1.7664e-02,  1.2847e-02],\n",
      "        [ 2.6118e-04, -2.4102e-01, -1.6138e-02,  2.2545e-01],\n",
      "        [-4.7101e-02, -2.3720e-01,  4.2779e-03,  1.4131e-01],\n",
      "        [ 1.6661e-02, -4.9025e-02, -2.3114e-02,  1.7859e-03],\n",
      "        [ 8.9912e-03, -4.3650e-01, -2.6667e-02,  5.2641e-01],\n",
      "        [ 1.3744e-02,  1.4584e-01, -1.7407e-02, -2.8536e-01],\n",
      "        [ 1.7488e-03, -4.3352e-01, -3.9647e-02,  4.6072e-01],\n",
      "        [ 9.8372e-03,  1.4700e-01, -1.7592e-02, -3.1089e-01],\n",
      "        [ 7.4203e-03, -4.4534e-02, -4.1339e-02, -9.7361e-02],\n",
      "        [-1.0293e-01, -1.0192e+00,  6.3226e-02,  1.3480e+00]])\n"
     ]
    }
   ],
   "source": [
    "for sample in gen:\n",
    "    obs_batch = sample[0]\n",
    "    print(obs_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    env.render()\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                torch.from_numpy(state).float(), None)\n",
    "            action\n",
    "            actual_action = 0\n",
    "            if action > 0.5:\n",
    "                actual_action = 1\n",
    "        state, reward, done, _ = env.step(actual_action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(episode_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 1000\n",
    "def moving_average(a, k=n):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[k:] = ret[k:] - ret[:-k]\n",
    "    return ret[k - 1:] / k\n",
    "plt.plot(list(range(len(action_losses)))[:1-n], moving_average(action_losses))\n",
    "plt.show()\n",
    "plt.plot(list(range(len(value_losses)))[:1-n], moving_average(value_losses))\n",
    "plt.show()\n",
    "plt.plot(list(range(len(dist_entropies)))[:1-n], moving_average(dist_entropies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
