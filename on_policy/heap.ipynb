{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from scipy.special import expit as sigmoid\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import time\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from a2c_ppo_acktr import utils\n",
    "import random\n",
    "\n",
    "from heap import Heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    bandwidth = 3\n",
    "    postwidth = 3\n",
    "    threshold = 0.8\n",
    "    slip_reward = -0.1\n",
    "    signal_split = 1\n",
    "    fps = 25\n",
    "    log_interval = 10\n",
    "    use_gae = False\n",
    "    num_updates = 1e5\n",
    "    num_steps = 32\n",
    "    memory_capacity = num_steps*fps + 1\n",
    "    clip_param = 0.2\n",
    "    ppo_epoch = 4\n",
    "    num_mini_batch = 32\n",
    "    value_loss_coef = 0.5\n",
    "    entropy_coef = 0.01\n",
    "    lr = 1e-2\n",
    "    eps = 1e-5\n",
    "    max_grad_norm = 0.2\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    use_proper_time_limits = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "heap = Heap(13, 4, 1, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1, Avg/Max/Min. reward: 12.0/12.0/12.0, Avg randomness: 0.92\n",
      "Iter: 2, Avg/Max/Min. reward: 18.5/25.0/12.0, Avg randomness: 0.95\n",
      "Iter: 3, Avg/Max/Min. reward: 18.3/25.0/12.0, Avg randomness: 0.95\n",
      "Iter: 4, Avg/Max/Min. reward: 20.0/25.0/12.0, Avg randomness: 0.95\n",
      "Iter: 5, Avg/Max/Min. reward: 21.2/26.0/12.0, Avg randomness: 0.95\n",
      "Iter: 6, Avg/Max/Min. reward: 19.3/26.0/10.0, Avg randomness: 0.95\n",
      "Iter: 7, Avg/Max/Min. reward: 19.6/26.0/10.0, Avg randomness: 0.95\n",
      "Iter: 8, Avg/Max/Min. reward: 21.1/32.0/10.0, Avg randomness: 0.96\n",
      "Iter: 9, Avg/Max/Min. reward: 23.8/53.0/10.0, Avg randomness: 0.96\n",
      "Iter: 10, Avg/Max/Min. reward: 23.4/53.0/10.0, Avg randomness: 0.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-93030e4a70d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeros_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mepisode_randoms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter: %d, Avg/Max/Min. reward: %0.1f/%0.1f/%0.1f, Avg randomness: %0.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_randoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/on_policy/heap.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/on_policy/unit.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.memory.compute_returns(next_value, self.args.use_gae, self.args.gamma,\n\u001b[1;32m     80\u001b[0m                                self.args.gae_lambda, self.args.use_proper_time_limits)\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/on_policy/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mvalue_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreturn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     value_losses_clipped = (\n\u001b[0;32m---> 73\u001b[0;31m                         value_pred_clipped - return_batch).pow(2)\n\u001b[0m\u001b[1;32m     74\u001b[0m                     value_loss = 0.5 * torch.max(value_losses,\n\u001b[1;32m     75\u001b[0m                                                  value_losses_clipped).mean()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = deque(maxlen=10)\n",
    "episode_randoms = deque(maxlen=10)\n",
    "done = True\n",
    "\n",
    "for j in count(1):\n",
    "    heap.clear_memory()\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_rewards.append(0)\n",
    "        episode_randoms.append(0)\n",
    "    for unit in heap.units:\n",
    "        utils.update_linear_schedule(unit.agent.optimizer, j, args.num_updates, args.lr)\n",
    "    for step in range(args.num_steps):\n",
    "        action, zeros_mask = heap(state, n=args.fps)\n",
    "        action = int(action > 0.5)\n",
    "        if len(zeros_mask[0]):\n",
    "            action = random.randint(0, 1)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_rewards[-1] += reward\n",
    "        if done:\n",
    "            heap.done()\n",
    "        elif len(zeros_mask[0]):\n",
    "            episode_randoms[-1] += 1\n",
    "    heap.update()\n",
    "    print('Iter: %d, Avg/Max/Min. reward: %0.1f/%0.1f/%0.1f, Avg randomness: %0.2f' % (j, sum(episode_rewards)/len(episode_rewards), max(episode_rewards), min(episode_rewards), sum(episode_randoms)/sum(episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap.plot_stats(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
