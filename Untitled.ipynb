{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sac import SAC\n",
    "from replay_memory import ReplayMemory\n",
    "import gym\n",
    "from gym import spaces\n",
    "from addict import Dict\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.special import softmax\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 0\n",
    "ACT_THRESHOLD = 0\n",
    "BATCH_SIZE = 256\n",
    "UPDATES_PER_STEP = 1\n",
    "PRINT_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_hidden = 8\n",
    "num_outputs = 1\n",
    "bandwidth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape = (1,)\n",
    "env.action_space.high = np.array([1])\n",
    "env.action_space.low = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_action_space = gym.spaces.Box(low=0, high=1, shape=(num_hidden + bandwidth,))\n",
    "hidden_layer_action_space = gym.spaces.Box(low=0, high=1, shape=(num_outputs + bandwidth,))\n",
    "output_layer_action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dict()\n",
    "args.gamma = 0.99\n",
    "args.tau = 0.005\n",
    "args.alpha = 0.2\n",
    "args.policy = 'Gaussian'\n",
    "args.target_update_interval = 10\n",
    "args.automatic_entropy_tuning = True\n",
    "args.cuda = False\n",
    "args.hidden_size = 256\n",
    "args.lr = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(1, input_layer_action_space, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = [SAC(1, input_layer_action_space, args) for _ in range(num_inputs)]\n",
    "input_memory = [ReplayMemory(1000000) for _ in range(num_inputs)]\n",
    "hidden_layer = [SAC(bandwidth, hidden_layer_action_space, args) for _ in range(num_hidden)]\n",
    "hidden_memory = [ReplayMemory(1000000) for _ in range(num_hidden)]\n",
    "output_layer = [SAC(bandwidth, output_layer_action_space, args) for _ in range(num_outputs)]\n",
    "output_memory = [ReplayMemory(1000000) for _ in range(num_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_layers(input_state):\n",
    "    input_state_actions = [tuple() for _ in range(num_inputs)]\n",
    "    hidden_state_actions = [tuple() for _ in range(num_hidden)]\n",
    "    output_state_actions = [tuple() for _ in range(num_outputs)]\n",
    "    input_actions = [agent.select_action(input_state[i:i+1]) for i, agent in enumerate(input_layer)]\n",
    "    for i, input_action in enumerate(input_actions):\n",
    "        input_action[:num_hidden] = softmax(input_action[:num_hidden])\n",
    "        if max(input_action[:num_hidden]) > ACT_THRESHOLD:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, True)\n",
    "            input_actions[i] = (np.argmax(input_action[:num_hidden]), input_action[num_hidden:])\n",
    "        else:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, False)\n",
    "            input_actions[i] = tuple()\n",
    "    hidden_state = [tuple() for _ in range(num_hidden)]\n",
    "    for input_action in input_actions:\n",
    "        try:\n",
    "            hidden_i, hidden_msg = input_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            hidden_state[hidden_i] += hidden_msg\n",
    "        except:\n",
    "            hidden_state[hidden_i] = hidden_msg\n",
    "    hidden_actions = []\n",
    "    for i, _hidden_state in enumerate(hidden_state):\n",
    "        if len(_hidden_state):\n",
    "            hidden_action = hidden_layer[i].select_action(hidden_state[i]) \n",
    "            if hidden_action[0] > ACT_THRESHOLD:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, True)\n",
    "                hidden_actions.append((0, hidden_action[1:]))\n",
    "            else:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, False)\n",
    "                hidden_actions.append(tuple())\n",
    "    output_state = [tuple() for _ in range(num_outputs)]\n",
    "    for hidden_action in hidden_actions:\n",
    "        try:\n",
    "            output_i, output_msg = hidden_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            output_state[output_i] += output_msg\n",
    "        except:\n",
    "            output_state[output_i] = output_msg\n",
    "    output_actions = [agent.select_action(output_state[i]) for i, agent in enumerate(output_layer) if len(output_state[i])]\n",
    "    if output_actions:\n",
    "        output_state_actions = [(output_state[0], output_actions[0], True)]\n",
    "    else:\n",
    "        output_state_actions = [tuple()]\n",
    "    inner_activations = {\n",
    "        'input_state_actions' : input_state_actions,\n",
    "        'hidden_state_actions' : hidden_state_actions,\n",
    "        'output_state_actions' : output_state_actions\n",
    "    }\n",
    "    try:\n",
    "        if output_actions[0] > 0.5:\n",
    "            return 1, inner_activations\n",
    "        else:\n",
    "            return 0, inner_activations\n",
    "    except:\n",
    "        return random.randint(0, 1), inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_layers(input_state):\n",
    "    input_state_actions = [tuple() for _ in range(num_inputs)]\n",
    "    hidden_state_actions = [tuple() for _ in range(num_hidden)]\n",
    "    output_state_actions = [tuple() for _ in range(num_outputs)]\n",
    "    input_actions = [input_layer_action_space.sample() for _ in input_layer]\n",
    "    for i, input_action in enumerate(input_actions):\n",
    "        input_action[:num_hidden] = softmax(input_action[:num_hidden])\n",
    "        if max(input_action[:num_hidden]) > ACT_THRESHOLD:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, True)\n",
    "            input_actions[i] = (np.argmax(input_action[:num_hidden]), input_action[num_hidden:])\n",
    "        else:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, False)\n",
    "            input_actions[i] = tuple()\n",
    "    hidden_state = [tuple() for _ in range(num_hidden)]\n",
    "    for input_action in input_actions:\n",
    "        try:\n",
    "            hidden_i, hidden_msg = input_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            hidden_state[hidden_i] += hidden_msg\n",
    "        except:\n",
    "            hidden_state[hidden_i] = hidden_msg\n",
    "    hidden_actions = []\n",
    "    for i, _hidden_state in enumerate(hidden_state):\n",
    "        if len(_hidden_state):\n",
    "            hidden_action = hidden_layer_action_space.sample()\n",
    "            if hidden_action[0] > ACT_THRESHOLD:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, True)\n",
    "                hidden_actions.append((0, hidden_action[1:]))\n",
    "            else:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, False)\n",
    "                hidden_actions.append(tuple())\n",
    "    output_state = [tuple() for _ in range(num_outputs)]\n",
    "    for hidden_action in hidden_actions:\n",
    "        try:\n",
    "            output_i, output_msg = hidden_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            output_state[output_i] += output_msg\n",
    "        except:\n",
    "            output_state[output_i] = output_msg\n",
    "    output_actions = [np.array([output_layer_action_space.sample()]) for i, _ in enumerate(output_layer) if len(output_state[i])]\n",
    "    output_state_actions = [(output_state[0], output_actions[0], True)]\n",
    "    inner_activations = {\n",
    "        'input_state_actions' : input_state_actions,\n",
    "        'hidden_state_actions' : hidden_state_actions,\n",
    "        'output_state_actions' : output_state_actions\n",
    "    }\n",
    "    try:\n",
    "        if output_actions[0] > 0.5:\n",
    "            return 1, inner_activations\n",
    "        else:\n",
    "            return 0, inner_activations\n",
    "    except:\n",
    "        return random.randint(0, 1), inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_memory(inner_activations, reward, next_state, mask):\n",
    "    input_state_actions = inner_activations['input_state_actions']\n",
    "    hidden_state_actions = inner_activations['hidden_state_actions']\n",
    "    output_state_actions = inner_activations['output_state_actions']\n",
    "    for i, ((input_state, input_action, flag), mem) in enumerate(zip(input_state_actions, input_memory)):\n",
    "        next_input_state = next_state[i:i+1]\n",
    "        if flag:\n",
    "            mem.push(input_state, input_action, reward, next_input_state, mask)\n",
    "        else:\n",
    "            mem.push(input_state, input_action, 0, next_input_state, mask)\n",
    "    next_action, next_inner_activations = eval_layers(next_state)\n",
    "    for hidden_state_action, mem, next_hidden_state_action in zip(hidden_state_actions, hidden_memory, next_inner_activations['hidden_state_actions']):\n",
    "        try:\n",
    "            hidden_state, hidden_action, flag = hidden_state_action\n",
    "            next_hidden_state, _, _ = next_hidden_state_action\n",
    "        except:\n",
    "            continue\n",
    "        if flag:\n",
    "            mem.push(hidden_state, hidden_action, reward, next_hidden_state, mask)\n",
    "        else:\n",
    "            mem.push(hidden_state, hidden_action, 0, next_hidden_state, mask)\n",
    "    for output_state_action, mem, next_output_state_action in zip(output_state_actions, output_memory, next_inner_activations['output_state_actions']):\n",
    "        try:\n",
    "            output_state, output_action, flag = output_state_action\n",
    "            next_output_state, _, _ = next_output_state_action\n",
    "        except:\n",
    "            continue\n",
    "        if flag:\n",
    "            mem.push(output_state, output_action, reward, next_output_state, mask)\n",
    "        else:\n",
    "            mem.push(output_state, output_action, 0, next_output_state, mask)\n",
    "    return next_action, next_inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_stats(stats):\n",
    "    for k in stats:\n",
    "        stats[k] /= stats.cnt\n",
    "    return stats\n",
    "def average_stats(lst):\n",
    "    avg = Dict()\n",
    "    for stats in lst:\n",
    "        for k in stats:\n",
    "            avg[k] += stats[k]\n",
    "        avg.cnt += 1\n",
    "    return norm_stats(avg)\n",
    "def get_avg_loss(train_stats):\n",
    "    try:\n",
    "        input_loss = (train_stats.input.critic_1_loss + train_stats.input.critic_2_loss + train_stats.input.policy_loss)/3\n",
    "    except:\n",
    "        input_loss = float('nan')\n",
    "    try:\n",
    "        hidden_loss = (train_stats.hidden.critic_1_loss + train_stats.hidden.critic_2_loss + train_stats.hidden.policy_loss)/3\n",
    "    except:\n",
    "        hidden_loss = float('nan')\n",
    "    try:\n",
    "        output_loss = (train_stats.output.critic_1_loss + train_stats.output.critic_2_loss + train_stats.output.policy_loss)/3\n",
    "    except:\n",
    "        output_loss = float('nan')\n",
    "    return input_loss, hidden_loss, output_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 1, total steps: 31, episode steps: 31, reward: 31.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 2, total steps: 50, episode steps: 19, reward: 19.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 3, total steps: 67, episode steps: 17, reward: 17.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 4, total steps: 86, episode steps: 19, reward: 19.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 5, total steps: 107, episode steps: 21, reward: 21.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 6, total steps: 125, episode steps: 18, reward: 18.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 7, total steps: 151, episode steps: 26, reward: 26.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 8, total steps: 160, episode steps: 9, reward: 9.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 9, total steps: 181, episode steps: 21, reward: 21.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 10, total steps: 198, episode steps: 17, reward: 17.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 16.0\n",
      "----------------------------------------\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 11, total steps: 220, episode steps: 22, reward: 22.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 12, total steps: 257, episode steps: 37, reward: 37.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "Episode: 13, total steps: 274, episode steps: 17, reward: 17.0, loss_i: -0.041, loss_h: nan, loss_o: 0.007\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7a7a9d0e6b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0minput_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mcritic_1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0minput_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_1_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcritic_1_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0minput_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcritic_2_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/sac.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "updates = 0\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    reasoned_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    inner_activations = {}\n",
    "    action = None\n",
    "    train_stats = Dict()\n",
    "    train_stats.input = []\n",
    "    train_stats.hidden = []\n",
    "    train_stats.output = []\n",
    "    while not done:\n",
    "        if WARM_UP > total_steps:\n",
    "            action, inner_activations = sample_layers(state)  # Sample random action\n",
    "        else:\n",
    "            if not inner_activations:\n",
    "                action, inner_activations = eval_layers(state)  # Sample action from policy\n",
    "            if inner_activations['output_state_actions'][0]:\n",
    "                reasoned_steps += 1\n",
    "\n",
    "        # Number of updates per step in environment\n",
    "        for i in range(UPDATES_PER_STEP):\n",
    "            # Update parameters of all the networks\n",
    "            flag = False\n",
    "            for agent, memory in zip(input_layer, input_memory):\n",
    "                input_stats = Dict()\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    input_stats.critic_1_loss += critic_1_loss\n",
    "                    input_stats.critic_2_loss += critic_2_loss\n",
    "                    input_stats.policy_loss += policy_loss\n",
    "                    input_stats.ent_loss += ent_loss\n",
    "                    input_stats.alpha += alpha\n",
    "                    input_stats.cnt += 1\n",
    "                    flag = True\n",
    "                norm_stats(input_stats)\n",
    "                train_stats.input.append(input_stats)\n",
    "            for agent, memory in zip(hidden_layer, hidden_memory):\n",
    "                hidden_stats = Dict()\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    hidden_stats.critic_1_loss += critic_1_loss\n",
    "                    hidden_stats.critic_2_loss += critic_2_loss\n",
    "                    hidden_stats.policy_loss += policy_loss\n",
    "                    hidden_stats.ent_loss += ent_loss\n",
    "                    hidden_stats.alpha += alpha\n",
    "                    hidden_stats.cnt += 1\n",
    "                    flag = True\n",
    "                norm_stats(hidden_stats)\n",
    "                train_stats.hidden.append(hidden_stats)\n",
    "            for agent, memory in zip(output_layer, output_memory):\n",
    "                output_stats = Dict()\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    output_stats.critic_1_loss += critic_1_loss\n",
    "                    output_stats.critic_2_loss += critic_2_loss\n",
    "                    output_stats.policy_loss += policy_loss\n",
    "                    output_stats.ent_loss += ent_loss\n",
    "                    output_stats.alpha += alpha\n",
    "                    output_stats.cnt += 1\n",
    "                    flag = True\n",
    "                norm_stats(output_stats)\n",
    "                train_stats.output.append(output_stats)\n",
    "            updates += flag\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 #if episode_steps == env._max_episode_steps else float(not done)\n",
    "        reward = reward - int(1 - mask)\n",
    "        action, inner_activations = push_memory(inner_activations, reward, next_state, mask)\n",
    "\n",
    "        state = next_state\n",
    "    train_stats.input = average_stats(train_stats.input)\n",
    "    train_stats.hidden = average_stats(train_stats.hidden)\n",
    "    train_stats.output = average_stats(train_stats.output)\n",
    "    loss = get_avg_loss(train_stats)\n",
    "    print(\"Episode: {}, total steps: {}, episode steps: {}, reward: {}, loss_i: {}, loss_h: {}, loss_o: {}\".format(i_episode, total_steps, episode_steps, round(episode_reward, 2), *list(map(lambda x : round(x, 3), loss))))\n",
    "    if updates > 0 and i_episode % PRINT_FREQ == 0:\n",
    "        print('INPUT: %s' % str(train_stats.input))\n",
    "        print('HIDDEN: %s' % str(train_stats.hidden))\n",
    "        print('OUTPUT: %s' % str(train_stats.output))\n",
    "    \n",
    "    if i_episode % 10 == 0:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = eval_layers(state)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
