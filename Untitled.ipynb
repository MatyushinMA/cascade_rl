{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sac import SAC\n",
    "from replay_memory import ReplayMemory\n",
    "import gym\n",
    "from gym import spaces\n",
    "from addict import Dict\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.special import softmax\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 10000\n",
    "ACT_THRESHOLD = 0\n",
    "BATCH_SIZE = 256\n",
    "UPDATES_PER_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_hidden = 8\n",
    "num_outputs = 1\n",
    "bandwidth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape = (1,)\n",
    "env.action_space.high = np.array([1])\n",
    "env.action_space.low = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_action_space = gym.spaces.Box(low=0, high=1, shape=(num_hidden + bandwidth,))\n",
    "hidden_layer_action_space = gym.spaces.Box(low=0, high=1, shape=(num_outputs + bandwidth,))\n",
    "output_layer_action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dict()\n",
    "args.gamma = 0.99\n",
    "args.tau = 0.005\n",
    "args.alpha = 0.2\n",
    "args.policy = 'Gaussian'\n",
    "args.target_update_interval = 1\n",
    "args.automatic_entropy_tuning = False\n",
    "args.cuda = False\n",
    "args.hidden_size = 256\n",
    "args.lr = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(1, input_layer_action_space, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = [SAC(1, input_layer_action_space, args) for _ in range(num_inputs)]\n",
    "input_memory = [ReplayMemory(1000000) for _ in range(num_inputs)]\n",
    "hidden_layer = [SAC(bandwidth, hidden_layer_action_space, args) for _ in range(num_hidden)]\n",
    "hidden_memory = [ReplayMemory(1000000) for _ in range(num_hidden)]\n",
    "output_layer = [SAC(bandwidth, output_layer_action_space, args) for _ in range(num_outputs)]\n",
    "output_memory = [ReplayMemory(1000000) for _ in range(num_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_layers(input_state):\n",
    "    input_state_actions = [tuple() for _ in range(num_inputs)]\n",
    "    hidden_state_actions = [tuple() for _ in range(num_hidden)]\n",
    "    output_state_actions = [tuple() for _ in range(num_outputs)]\n",
    "    input_actions = [agent.select_action(input_state[i:i+1]) for i, agent in enumerate(input_layer)]\n",
    "    for i, input_action in enumerate(input_actions):\n",
    "        input_action[:num_hidden] = softmax(input_action[:num_hidden])\n",
    "        if max(input_action[:num_hidden]) > ACT_THRESHOLD:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, True)\n",
    "            input_actions[i] = (np.argmax(input_action[:num_hidden]), input_action[num_hidden:])\n",
    "        else:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, False)\n",
    "            input_actions[i] = tuple()\n",
    "    hidden_state = [tuple() for _ in range(num_hidden)]\n",
    "    for input_action in input_actions:\n",
    "        try:\n",
    "            hidden_i, hidden_msg = input_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            hidden_state[hidden_i] += hidden_msg\n",
    "        except:\n",
    "            hidden_state[hidden_i] = hidden_msg\n",
    "    hidden_actions = []\n",
    "    for i, _hidden_state in enumerate(hidden_state):\n",
    "        if len(_hidden_state):\n",
    "            hidden_action = hidden_layer[i].select_action(hidden_state[i]) \n",
    "            if hidden_action[0] > ACT_THRESHOLD:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, True)\n",
    "                hidden_actions.append((0, hidden_action[1:]))\n",
    "            else:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, False)\n",
    "                hidden_actions.append(tuple())\n",
    "    output_state = [tuple() for _ in range(num_outputs)]\n",
    "    for hidden_action in hidden_actions:\n",
    "        try:\n",
    "            output_i, output_msg = hidden_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            output_state[output_i] += output_msg\n",
    "        except:\n",
    "            output_state[output_i] = output_msg\n",
    "    output_actions = [agent.select_action(output_state[i]) for i, agent in enumerate(output_layer) if len(output_state[i])]\n",
    "    if output_actions:\n",
    "        output_state_actions = [(output_state[0], output_actions[0], True)]\n",
    "    else:\n",
    "        output_state_actions = [tuple()]\n",
    "    inner_activations = {\n",
    "        'input_state_actions' : input_state_actions,\n",
    "        'hidden_state_actions' : hidden_state_actions,\n",
    "        'output_state_actions' : output_state_actions\n",
    "    }\n",
    "    try:\n",
    "        if output_actions[0] > 0.5:\n",
    "            return 1, inner_activations\n",
    "        else:\n",
    "            return 0, inner_activations\n",
    "    except:\n",
    "        return random.randint(0, 1), inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_memory(inner_activations, reward, next_state, mask):\n",
    "    input_state_actions = inner_activations['input_state_actions']\n",
    "    hidden_state_actions = inner_activations['hidden_state_actions']\n",
    "    output_state_actions = inner_activations['output_state_actions']\n",
    "    for i, ((input_state, input_action, flag), mem) in enumerate(zip(input_state_actions, input_memory)):\n",
    "        next_input_state = next_state[i:i+1]\n",
    "        if flag:\n",
    "            mem.push(input_state, input_action, reward, next_input_state, mask)\n",
    "        else:\n",
    "            mem.push(input_state, input_action, 0, next_input_state, mask)\n",
    "    next_action, next_inner_activations = eval_layers(next_state)\n",
    "    for hidden_state_action, mem, next_hidden_state_action in zip(hidden_state_actions, hidden_memory, next_inner_activations['hidden_state_actions']):\n",
    "        try:\n",
    "            hidden_state, hidden_action, flag = hidden_state_action\n",
    "            next_hidden_state, _, _ = next_hidden_state_action\n",
    "        except:\n",
    "            continue\n",
    "        if flag:\n",
    "            mem.push(hidden_state, hidden_action, reward, next_hidden_state, mask)\n",
    "        else:\n",
    "            mem.push(hidden_state, hidden_action, 0, next_hidden_state, mask)\n",
    "    for output_state_action, mem, next_output_state_action in zip(output_state_actions, output_memory, next_inner_activations['output_state_actions']):\n",
    "        try:\n",
    "            output_state, output_action, flag = output_state_action\n",
    "            next_output_state, _, _ = next_output_state_action\n",
    "        except:\n",
    "            continue\n",
    "        if flag:\n",
    "            mem.push(output_state, output_action, reward, next_output_state, mask)\n",
    "        else:\n",
    "            mem.push(output_state, output_action, 0, next_output_state, mask)\n",
    "    return next_action, next_inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, total steps: 25, episode steps: 25, reasoned steps: 0, reward: 25.0\n",
      "Episode: 2, total steps: 37, episode steps: 12, reasoned steps: 0, reward: 12.0\n",
      "Episode: 3, total steps: 54, episode steps: 17, reasoned steps: 0, reward: 17.0\n",
      "Episode: 4, total steps: 71, episode steps: 17, reasoned steps: 0, reward: 17.0\n",
      "Episode: 5, total steps: 98, episode steps: 27, reasoned steps: 0, reward: 27.0\n",
      "Episode: 6, total steps: 146, episode steps: 48, reasoned steps: 0, reward: 48.0\n",
      "Episode: 7, total steps: 164, episode steps: 18, reasoned steps: 0, reward: 18.0\n",
      "Episode: 8, total steps: 178, episode steps: 14, reasoned steps: 0, reward: 14.0\n",
      "Episode: 9, total steps: 193, episode steps: 15, reasoned steps: 0, reward: 15.0\n",
      "Episode: 10, total steps: 230, episode steps: 37, reasoned steps: 0, reward: 37.0\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 26.3\n",
      "----------------------------------------\n",
      "Episode: 11, total steps: 244, episode steps: 14, reasoned steps: 0, reward: 14.0\n",
      "Episode: 12, total steps: 254, episode steps: 10, reasoned steps: 0, reward: 10.0\n",
      "Episode: 13, total steps: 281, episode steps: 27, reasoned steps: 0, reward: 27.0\n",
      "Episode: 14, total steps: 292, episode steps: 11, reasoned steps: 0, reward: 11.0\n",
      "Episode: 15, total steps: 306, episode steps: 14, reasoned steps: 0, reward: 14.0\n",
      "Episode: 16, total steps: 340, episode steps: 34, reasoned steps: 0, reward: 34.0\n",
      "Episode: 17, total steps: 355, episode steps: 15, reasoned steps: 0, reward: 15.0\n",
      "Episode: 18, total steps: 390, episode steps: 35, reasoned steps: 0, reward: 35.0\n",
      "Episode: 19, total steps: 412, episode steps: 22, reasoned steps: 0, reward: 22.0\n",
      "Episode: 20, total steps: 447, episode steps: 35, reasoned steps: 0, reward: 35.0\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 32.1\n",
      "----------------------------------------\n",
      "Episode: 21, total steps: 472, episode steps: 25, reasoned steps: 0, reward: 25.0\n",
      "Episode: 22, total steps: 502, episode steps: 30, reasoned steps: 0, reward: 30.0\n",
      "Episode: 23, total steps: 527, episode steps: 25, reasoned steps: 0, reward: 25.0\n",
      "Episode: 24, total steps: 544, episode steps: 17, reasoned steps: 0, reward: 17.0\n",
      "Episode: 25, total steps: 554, episode steps: 10, reasoned steps: 0, reward: 10.0\n",
      "Episode: 26, total steps: 602, episode steps: 48, reasoned steps: 0, reward: 48.0\n",
      "Episode: 27, total steps: 623, episode steps: 21, reasoned steps: 0, reward: 21.0\n",
      "Episode: 28, total steps: 636, episode steps: 13, reasoned steps: 0, reward: 13.0\n",
      "Episode: 29, total steps: 651, episode steps: 15, reasoned steps: 0, reward: 15.0\n",
      "Episode: 30, total steps: 663, episode steps: 12, reasoned steps: 0, reward: 12.0\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.8\n",
      "----------------------------------------\n",
      "Episode: 31, total steps: 672, episode steps: 9, reasoned steps: 0, reward: 9.0\n",
      "Episode: 32, total steps: 692, episode steps: 20, reasoned steps: 0, reward: 20.0\n",
      "Episode: 33, total steps: 705, episode steps: 13, reasoned steps: 0, reward: 13.0\n",
      "Episode: 34, total steps: 720, episode steps: 15, reasoned steps: 0, reward: 15.0\n",
      "Episode: 35, total steps: 732, episode steps: 12, reasoned steps: 0, reward: 12.0\n",
      "Episode: 36, total steps: 762, episode steps: 30, reasoned steps: 0, reward: 30.0\n",
      "Episode: 37, total steps: 776, episode steps: 14, reasoned steps: 0, reward: 14.0\n",
      "Episode: 38, total steps: 790, episode steps: 14, reasoned steps: 0, reward: 14.0\n",
      "Episode: 39, total steps: 801, episode steps: 11, reasoned steps: 0, reward: 11.0\n",
      "Episode: 40, total steps: 814, episode steps: 13, reasoned steps: 0, reward: 13.0\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 16.4\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "updates = 0\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    reasoned_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    inner_activations = {}\n",
    "    action = None\n",
    "    while not done:\n",
    "        if WARM_UP > total_steps:\n",
    "            action = env.action_space.sample()  # Sample random action\n",
    "            _, inner_activations = eval_layers(state)\n",
    "        else:\n",
    "            if not inner_activations:\n",
    "                action, inner_activations = eval_layers(state)  # Sample action from policy\n",
    "            if inner_activations['output_state_actions'][0]:\n",
    "                reasoned_steps += 1\n",
    "\n",
    "        # Number of updates per step in environment\n",
    "        for i in range(UPDATES_PER_STEP):\n",
    "            # Update parameters of all the networks\n",
    "            flag = False\n",
    "            for agent, memory in zip(input_layer + hidden_layer + output_layer, input_memory + hidden_memory + output_memory):\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    _ = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    flag = True\n",
    "            updates += flag\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "        action, inner_activations = push_memory(inner_activations, reward, next_state, mask)\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    print(\"Episode: {}, total steps: {}, episode steps: {}, reasoned steps: {}, reward: {}\".format(i_episode, total_steps, episode_steps, reasoned_steps, round(episode_reward, 2)))\n",
    "    \n",
    "    if i_episode % 10 == 0:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = eval_layers(state)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
