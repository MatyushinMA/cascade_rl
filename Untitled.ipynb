{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sac import SAC\n",
    "from replay_memory import ReplayMemory\n",
    "import gym\n",
    "from gym import spaces\n",
    "from addict import Dict\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.special import softmax\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 0\n",
    "ACT_THRESHOLD = 0\n",
    "BATCH_SIZE = 256\n",
    "UPDATES_PER_STEP = 1\n",
    "PRINT_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_hidden = 8\n",
    "num_outputs = 1\n",
    "bandwidth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape = (1,)\n",
    "env.action_space.high = np.array([1])\n",
    "env.action_space.low = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_action_space = gym.spaces.Box(low=0, high=1, shape=(num_hidden + bandwidth,))\n",
    "hidden_layer_action_space = gym.spaces.Box(low=0, high=1, shape=(num_outputs + bandwidth,))\n",
    "output_layer_action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dict()\n",
    "args.gamma = 0.99\n",
    "args.tau = 0.005\n",
    "args.alpha = 0.2\n",
    "args.policy = 'Gaussian'\n",
    "args.target_update_interval = 10\n",
    "args.automatic_entropy_tuning = True\n",
    "args.cuda = False\n",
    "args.hidden_size = 256\n",
    "args.lr = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(1, input_layer_action_space, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = [SAC(1, input_layer_action_space, args) for _ in range(num_inputs)]\n",
    "input_memory = [ReplayMemory(1000000) for _ in range(num_inputs)]\n",
    "hidden_layer = [SAC(bandwidth, hidden_layer_action_space, args) for _ in range(num_hidden)]\n",
    "hidden_memory = [ReplayMemory(1000000) for _ in range(num_hidden)]\n",
    "output_layer = [SAC(bandwidth, output_layer_action_space, args) for _ in range(num_outputs)]\n",
    "output_memory = [ReplayMemory(1000000) for _ in range(num_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_layers(input_state):\n",
    "    input_state_actions = [tuple() for _ in range(num_inputs)]\n",
    "    hidden_state_actions = [tuple() for _ in range(num_hidden)]\n",
    "    output_state_actions = [tuple() for _ in range(num_outputs)]\n",
    "    input_actions = [agent.select_action(input_state[i:i+1]) for i, agent in enumerate(input_layer)]\n",
    "    for i, input_action in enumerate(input_actions):\n",
    "        input_action[:num_hidden] = softmax(input_action[:num_hidden])\n",
    "        if max(input_action[:num_hidden]) > ACT_THRESHOLD:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, True)\n",
    "            input_actions[i] = (np.argmax(input_action[:num_hidden]), input_action[num_hidden:])\n",
    "        else:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, False)\n",
    "            input_actions[i] = tuple()\n",
    "    hidden_state = [tuple() for _ in range(num_hidden)]\n",
    "    for input_action in input_actions:\n",
    "        try:\n",
    "            hidden_i, hidden_msg = input_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            hidden_state[hidden_i] += hidden_msg\n",
    "        except:\n",
    "            hidden_state[hidden_i] = hidden_msg\n",
    "    hidden_actions = []\n",
    "    for i, _hidden_state in enumerate(hidden_state):\n",
    "        if len(_hidden_state):\n",
    "            hidden_action = hidden_layer[i].select_action(hidden_state[i]) \n",
    "            if hidden_action[0] > ACT_THRESHOLD:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, True)\n",
    "                hidden_actions.append((0, hidden_action[1:]))\n",
    "            else:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, False)\n",
    "                hidden_actions.append(tuple())\n",
    "    output_state = [tuple() for _ in range(num_outputs)]\n",
    "    for hidden_action in hidden_actions:\n",
    "        try:\n",
    "            output_i, output_msg = hidden_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            output_state[output_i] += output_msg\n",
    "        except:\n",
    "            output_state[output_i] = output_msg\n",
    "    output_actions = [agent.select_action(output_state[i]) for i, agent in enumerate(output_layer) if len(output_state[i])]\n",
    "    if output_actions:\n",
    "        output_state_actions = [(output_state[0], output_actions[0], True)]\n",
    "    else:\n",
    "        output_state_actions = [tuple()]\n",
    "    inner_activations = {\n",
    "        'input_state_actions' : input_state_actions,\n",
    "        'hidden_state_actions' : hidden_state_actions,\n",
    "        'output_state_actions' : output_state_actions\n",
    "    }\n",
    "    try:\n",
    "        if output_actions[0] > 0.5:\n",
    "            return 1, inner_activations\n",
    "        else:\n",
    "            return 0, inner_activations\n",
    "    except:\n",
    "        return random.randint(0, 1), inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_layers(input_state):\n",
    "    input_state_actions = [tuple() for _ in range(num_inputs)]\n",
    "    hidden_state_actions = [tuple() for _ in range(num_hidden)]\n",
    "    output_state_actions = [tuple() for _ in range(num_outputs)]\n",
    "    input_actions = [input_layer_action_space.sample() for _ in input_layer]\n",
    "    for i, input_action in enumerate(input_actions):\n",
    "        input_action[:num_hidden] = softmax(input_action[:num_hidden])\n",
    "        if max(input_action[:num_hidden]) > ACT_THRESHOLD:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, True)\n",
    "            input_actions[i] = (np.argmax(input_action[:num_hidden]), input_action[num_hidden:])\n",
    "        else:\n",
    "            input_state_actions[i] = (input_state[i:i+1], input_action, False)\n",
    "            input_actions[i] = tuple()\n",
    "    hidden_state = [tuple() for _ in range(num_hidden)]\n",
    "    for input_action in input_actions:\n",
    "        try:\n",
    "            hidden_i, hidden_msg = input_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            hidden_state[hidden_i] += hidden_msg\n",
    "        except:\n",
    "            hidden_state[hidden_i] = hidden_msg\n",
    "    hidden_actions = []\n",
    "    for i, _hidden_state in enumerate(hidden_state):\n",
    "        if len(_hidden_state):\n",
    "            hidden_action = hidden_layer_action_space.sample()\n",
    "            if hidden_action[0] > ACT_THRESHOLD:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, True)\n",
    "                hidden_actions.append((0, hidden_action[1:]))\n",
    "            else:\n",
    "                hidden_state_actions[i] = (_hidden_state, hidden_action, False)\n",
    "                hidden_actions.append(tuple())\n",
    "    output_state = [tuple() for _ in range(num_outputs)]\n",
    "    for hidden_action in hidden_actions:\n",
    "        try:\n",
    "            output_i, output_msg = hidden_action\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            output_state[output_i] += output_msg\n",
    "        except:\n",
    "            output_state[output_i] = output_msg\n",
    "    output_actions = [np.array([output_layer_action_space.sample()]) for i, _ in enumerate(output_layer) if len(output_state[i])]\n",
    "    output_state_actions = [(output_state[0], output_actions[0], True)]\n",
    "    inner_activations = {\n",
    "        'input_state_actions' : input_state_actions,\n",
    "        'hidden_state_actions' : hidden_state_actions,\n",
    "        'output_state_actions' : output_state_actions\n",
    "    }\n",
    "    try:\n",
    "        if output_actions[0] > 0.5:\n",
    "            return 1, inner_activations\n",
    "        else:\n",
    "            return 0, inner_activations\n",
    "    except:\n",
    "        return random.randint(0, 1), inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_memory(inner_activations, reward, next_state, mask):\n",
    "    input_state_actions = inner_activations['input_state_actions']\n",
    "    hidden_state_actions = inner_activations['hidden_state_actions']\n",
    "    output_state_actions = inner_activations['output_state_actions']\n",
    "    for i, ((input_state, input_action, flag), mem) in enumerate(zip(input_state_actions, input_memory)):\n",
    "        next_input_state = next_state[i:i+1]\n",
    "        if flag:\n",
    "            mem.push(input_state, input_action, reward, next_input_state, mask)\n",
    "        else:\n",
    "            mem.push(input_state, input_action, 0, next_input_state, mask)\n",
    "    next_action, next_inner_activations = eval_layers(next_state)\n",
    "    for hidden_state_action, mem, next_hidden_state_action in zip(hidden_state_actions, hidden_memory, next_inner_activations['hidden_state_actions']):\n",
    "        try:\n",
    "            hidden_state, hidden_action, flag = hidden_state_action\n",
    "            next_hidden_state, _, _ = next_hidden_state_action\n",
    "        except:\n",
    "            continue\n",
    "        if flag:\n",
    "            mem.push(hidden_state, hidden_action, reward, next_hidden_state, mask)\n",
    "        else:\n",
    "            mem.push(hidden_state, hidden_action, 0, next_hidden_state, mask)\n",
    "    for output_state_action, mem, next_output_state_action in zip(output_state_actions, output_memory, next_inner_activations['output_state_actions']):\n",
    "        try:\n",
    "            output_state, output_action, flag = output_state_action\n",
    "            next_output_state, _, _ = next_output_state_action\n",
    "        except:\n",
    "            continue\n",
    "        if flag:\n",
    "            mem.push(output_state, output_action, reward, next_output_state, mask)\n",
    "        else:\n",
    "            mem.push(output_state, output_action, 0, next_output_state, mask)\n",
    "    return next_action, next_inner_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_stats(stats):\n",
    "    for k in stats:\n",
    "        stats[k] /= stats.cnt\n",
    "    return stats\n",
    "def average_stats(lst):\n",
    "    avg = Dict()\n",
    "    for stats in lst:\n",
    "        for k in stats:\n",
    "            avg[k] += stats[k]\n",
    "        avg.cnt += 1\n",
    "    return norm_stats(avg)\n",
    "def get_avg_loss(train_stats):\n",
    "    try:\n",
    "        input_loss = (train_stats.input.critic_1_loss + train_stats.input.critic_2_loss + train_stats.input.policy_loss)/3\n",
    "    except:\n",
    "        input_loss = float('nan')\n",
    "    try:\n",
    "        hidden_loss = (train_stats.hidden.critic_1_loss + train_stats.hidden.critic_2_loss + train_stats.hidden.policy_loss)/3\n",
    "    except:\n",
    "        hidden_loss = float('nan')\n",
    "    try:\n",
    "        output_loss = (train_stats.output.critic_1_loss + train_stats.output.critic_2_loss + train_stats.output.policy_loss)/3\n",
    "    except:\n",
    "        output_loss = float('nan')\n",
    "    return input_loss, hidden_loss, output_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, total steps: 10, episode steps: 10, reward: 10.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 2, total steps: 28, episode steps: 18, reward: 18.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 3, total steps: 41, episode steps: 13, reward: 13.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 4, total steps: 58, episode steps: 17, reward: 17.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 5, total steps: 68, episode steps: 10, reward: 10.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 6, total steps: 78, episode steps: 10, reward: 10.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 7, total steps: 120, episode steps: 42, reward: 42.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 8, total steps: 179, episode steps: 59, reward: 59.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 9, total steps: 189, episode steps: 10, reward: 10.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 10, total steps: 203, episode steps: 14, reward: 14.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 27.3\n",
      "----------------------------------------\n",
      "Episode: 11, total steps: 228, episode steps: 25, reward: 25.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 12, total steps: 243, episode steps: 15, reward: 15.0, loss_i: nan, loss_h: nan, loss_o: nan\n",
      "Episode: 13, total steps: 260, episode steps: 17, reward: 17.0, loss_i: 2.624, loss_h: nan, loss_o: 2.802\n",
      "Episode: 14, total steps: 272, episode steps: 12, reward: 12.0, loss_i: -0.081, loss_h: nan, loss_o: 0.016\n",
      "Episode: 15, total steps: 284, episode steps: 12, reward: 12.0, loss_i: -0.203, loss_h: nan, loss_o: -0.104\n",
      "Episode: 16, total steps: 293, episode steps: 9, reward: 9.0, loss_i: -0.267, loss_h: nan, loss_o: -0.137\n",
      "Episode: 17, total steps: 308, episode steps: 15, reward: 15.0, loss_i: -0.281, loss_h: nan, loss_o: -0.148\n",
      "Episode: 18, total steps: 324, episode steps: 16, reward: 16.0, loss_i: -0.301, loss_h: nan, loss_o: -0.151\n",
      "Episode: 19, total steps: 338, episode steps: 14, reward: 14.0, loss_i: -0.313, loss_h: nan, loss_o: -0.154\n",
      "Episode: 20, total steps: 350, episode steps: 12, reward: 12.0, loss_i: -0.317, loss_h: nan, loss_o: -0.156\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 25.4\n",
      "----------------------------------------\n",
      "Episode: 21, total steps: 383, episode steps: 33, reward: 33.0, loss_i: -0.317, loss_h: nan, loss_o: -0.164\n",
      "Episode: 22, total steps: 405, episode steps: 22, reward: 22.0, loss_i: -0.341, loss_h: nan, loss_o: -0.168\n",
      "Episode: 23, total steps: 421, episode steps: 16, reward: 16.0, loss_i: -0.349, loss_h: nan, loss_o: -0.167\n",
      "Episode: 24, total steps: 443, episode steps: 22, reward: 22.0, loss_i: -0.358, loss_h: nan, loss_o: -0.177\n",
      "Episode: 25, total steps: 453, episode steps: 10, reward: 10.0, loss_i: -0.363, loss_h: nan, loss_o: -0.18\n",
      "Episode: 26, total steps: 478, episode steps: 25, reward: 25.0, loss_i: -0.371, loss_h: nan, loss_o: -0.185\n",
      "Episode: 27, total steps: 500, episode steps: 22, reward: 22.0, loss_i: -0.377, loss_h: nan, loss_o: -0.186\n",
      "Episode: 28, total steps: 515, episode steps: 15, reward: 15.0, loss_i: -0.389, loss_h: nan, loss_o: -0.19\n",
      "Episode: 29, total steps: 535, episode steps: 20, reward: 20.0, loss_i: -0.394, loss_h: nan, loss_o: -0.193\n",
      "Episode: 30, total steps: 550, episode steps: 15, reward: 15.0, loss_i: -0.405, loss_h: nan, loss_o: -0.192\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.7\n",
      "----------------------------------------\n",
      "Episode: 31, total steps: 569, episode steps: 19, reward: 19.0, loss_i: -0.413, loss_h: nan, loss_o: -0.196\n",
      "Episode: 32, total steps: 580, episode steps: 11, reward: 11.0, loss_i: -0.416, loss_h: nan, loss_o: -0.199\n",
      "Episode: 33, total steps: 607, episode steps: 27, reward: 27.0, loss_i: -0.421, loss_h: nan, loss_o: -0.196\n",
      "Episode: 34, total steps: 639, episode steps: 32, reward: 32.0, loss_i: -0.428, loss_h: nan, loss_o: -0.199\n",
      "Episode: 35, total steps: 657, episode steps: 18, reward: 18.0, loss_i: -0.441, loss_h: nan, loss_o: -0.201\n",
      "Episode: 36, total steps: 668, episode steps: 11, reward: 11.0, loss_i: -0.428, loss_h: nan, loss_o: -0.204\n",
      "Episode: 37, total steps: 690, episode steps: 22, reward: 22.0, loss_i: -0.434, loss_h: nan, loss_o: -0.208\n",
      "Episode: 38, total steps: 704, episode steps: 14, reward: 14.0, loss_i: -0.444, loss_h: nan, loss_o: -0.207\n",
      "Episode: 39, total steps: 757, episode steps: 53, reward: 53.0, loss_i: -0.457, loss_h: nan, loss_o: -0.21\n",
      "Episode: 40, total steps: 775, episode steps: 18, reward: 18.0, loss_i: -0.458, loss_h: nan, loss_o: -0.209\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.3\n",
      "----------------------------------------\n",
      "Episode: 41, total steps: 790, episode steps: 15, reward: 15.0, loss_i: -0.478, loss_h: nan, loss_o: -0.21\n",
      "Episode: 42, total steps: 835, episode steps: 45, reward: 45.0, loss_i: -0.473, loss_h: nan, loss_o: -0.211\n",
      "Episode: 43, total steps: 848, episode steps: 13, reward: 13.0, loss_i: -0.489, loss_h: nan, loss_o: -0.209\n",
      "Episode: 44, total steps: 878, episode steps: 30, reward: 30.0, loss_i: -0.483, loss_h: nan, loss_o: -0.213\n",
      "Episode: 45, total steps: 895, episode steps: 17, reward: 17.0, loss_i: -0.486, loss_h: nan, loss_o: -0.216\n",
      "Episode: 46, total steps: 914, episode steps: 19, reward: 19.0, loss_i: -0.494, loss_h: nan, loss_o: -0.216\n",
      "Episode: 47, total steps: 931, episode steps: 17, reward: 17.0, loss_i: -0.493, loss_h: nan, loss_o: -0.216\n",
      "Episode: 48, total steps: 979, episode steps: 48, reward: 48.0, loss_i: -0.509, loss_h: nan, loss_o: -0.219\n",
      "Episode: 49, total steps: 992, episode steps: 13, reward: 13.0, loss_i: -0.52, loss_h: nan, loss_o: -0.221\n",
      "Episode: 50, total steps: 1015, episode steps: 23, reward: 23.0, loss_i: -0.521, loss_h: nan, loss_o: -0.221\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 19.9\n",
      "----------------------------------------\n",
      "Episode: 51, total steps: 1026, episode steps: 11, reward: 11.0, loss_i: -0.529, loss_h: nan, loss_o: -0.224\n",
      "Episode: 52, total steps: 1051, episode steps: 25, reward: 25.0, loss_i: -0.531, loss_h: nan, loss_o: -0.225\n",
      "Episode: 53, total steps: 1071, episode steps: 20, reward: 20.0, loss_i: -0.541, loss_h: nan, loss_o: -0.224\n",
      "Episode: 54, total steps: 1090, episode steps: 19, reward: 19.0, loss_i: -0.54, loss_h: nan, loss_o: -0.225\n",
      "Episode: 55, total steps: 1105, episode steps: 15, reward: 15.0, loss_i: -0.541, loss_h: nan, loss_o: -0.225\n",
      "Episode: 56, total steps: 1124, episode steps: 19, reward: 19.0, loss_i: -0.543, loss_h: nan, loss_o: -0.226\n",
      "Episode: 57, total steps: 1145, episode steps: 21, reward: 21.0, loss_i: -0.543, loss_h: nan, loss_o: -0.226\n",
      "Episode: 58, total steps: 1157, episode steps: 12, reward: 12.0, loss_i: -0.544, loss_h: nan, loss_o: -0.227\n",
      "Episode: 59, total steps: 1174, episode steps: 17, reward: 17.0, loss_i: -0.538, loss_h: nan, loss_o: -0.227\n",
      "Episode: 60, total steps: 1207, episode steps: 33, reward: 33.0, loss_i: -0.544, loss_h: nan, loss_o: -0.225\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 26.2\n",
      "----------------------------------------\n",
      "Episode: 61, total steps: 1222, episode steps: 15, reward: 15.0, loss_i: -0.55, loss_h: nan, loss_o: -0.219\n",
      "Episode: 62, total steps: 1232, episode steps: 10, reward: 10.0, loss_i: -0.553, loss_h: nan, loss_o: -0.222\n",
      "Episode: 63, total steps: 1245, episode steps: 13, reward: 13.0, loss_i: -0.557, loss_h: nan, loss_o: -0.222\n",
      "Episode: 64, total steps: 1273, episode steps: 28, reward: 28.0, loss_i: -0.566, loss_h: nan, loss_o: -0.226\n",
      "Episode: 65, total steps: 1293, episode steps: 20, reward: 20.0, loss_i: -0.569, loss_h: nan, loss_o: -0.231\n",
      "Episode: 66, total steps: 1326, episode steps: 33, reward: 33.0, loss_i: -0.575, loss_h: 1.409, loss_o: -0.232\n",
      "Episode: 67, total steps: 1360, episode steps: 34, reward: 34.0, loss_i: -0.573, loss_h: -2.848, loss_o: -0.234\n",
      "Episode: 68, total steps: 1380, episode steps: 20, reward: 20.0, loss_i: -0.574, loss_h: -4.64, loss_o: -0.234\n",
      "Episode: 69, total steps: 1388, episode steps: 8, reward: 8.0, loss_i: -0.582, loss_h: -2.081, loss_o: -0.235\n",
      "Episode: 70, total steps: 1401, episode steps: 13, reward: 13.0, loss_i: -0.567, loss_h: -2.629, loss_o: -0.234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 18.0\n",
      "----------------------------------------\n",
      "Episode: 71, total steps: 1411, episode steps: 10, reward: 10.0, loss_i: -0.573, loss_h: -3.334, loss_o: -0.235\n",
      "Episode: 72, total steps: 1448, episode steps: 37, reward: 37.0, loss_i: -0.573, loss_h: -23.075, loss_o: -0.236\n",
      "Episode: 73, total steps: 1473, episode steps: 25, reward: 25.0, loss_i: -0.575, loss_h: -19.477, loss_o: -0.236\n",
      "Episode: 74, total steps: 1489, episode steps: 16, reward: 16.0, loss_i: -0.577, loss_h: -15.341, loss_o: -0.238\n",
      "Episode: 75, total steps: 1505, episode steps: 16, reward: 16.0, loss_i: -0.579, loss_h: -20.579, loss_o: -0.238\n",
      "Episode: 76, total steps: 1520, episode steps: 15, reward: 15.0, loss_i: -0.571, loss_h: -20.235, loss_o: -0.24\n",
      "Episode: 77, total steps: 1536, episode steps: 16, reward: 16.0, loss_i: -0.577, loss_h: -22.843, loss_o: -0.24\n",
      "Episode: 78, total steps: 1547, episode steps: 11, reward: 11.0, loss_i: -0.571, loss_h: -15.97, loss_o: -0.24\n",
      "Episode: 79, total steps: 1573, episode steps: 26, reward: 26.0, loss_i: -0.578, loss_h: -38.597, loss_o: -0.241\n",
      "Episode: 80, total steps: 1584, episode steps: 11, reward: 11.0, loss_i: -0.592, loss_h: -16.851, loss_o: -0.244\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 19.5\n",
      "----------------------------------------\n",
      "Episode: 81, total steps: 1613, episode steps: 29, reward: 29.0, loss_i: -0.585, loss_h: -45.058, loss_o: -0.244\n",
      "Episode: 82, total steps: 1646, episode steps: 33, reward: 33.0, loss_i: -0.586, loss_h: -58.005, loss_o: -0.244\n",
      "Episode: 83, total steps: 1675, episode steps: 29, reward: 29.0, loss_i: -0.582, loss_h: -59.42, loss_o: -0.246\n",
      "Episode: 84, total steps: 1694, episode steps: 19, reward: 19.0, loss_i: -0.563, loss_h: -41.0, loss_o: -0.246\n",
      "Episode: 85, total steps: 1711, episode steps: 17, reward: 17.0, loss_i: -0.569, loss_h: -37.919, loss_o: -0.247\n",
      "Episode: 86, total steps: 1725, episode steps: 14, reward: 14.0, loss_i: -0.574, loss_h: -32.07, loss_o: -0.248\n",
      "Episode: 87, total steps: 1741, episode steps: 16, reward: 16.0, loss_i: -0.565, loss_h: -37.076, loss_o: -0.248\n",
      "Episode: 88, total steps: 1757, episode steps: 16, reward: 16.0, loss_i: -0.574, loss_h: -35.276, loss_o: -0.247\n",
      "Episode: 89, total steps: 1775, episode steps: 18, reward: 18.0, loss_i: -0.571, loss_h: -0.153, loss_o: -0.247\n",
      "Episode: 90, total steps: 1786, episode steps: 11, reward: 11.0, loss_i: -0.565, loss_h: -0.159, loss_o: -0.251\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 13.2\n",
      "----------------------------------------\n",
      "Episode: 91, total steps: 1798, episode steps: 12, reward: 12.0, loss_i: -0.565, loss_h: -0.161, loss_o: -0.247\n",
      "Episode: 92, total steps: 1811, episode steps: 13, reward: 13.0, loss_i: -0.573, loss_h: -0.165, loss_o: -0.247\n",
      "Episode: 93, total steps: 1835, episode steps: 24, reward: 24.0, loss_i: -0.571, loss_h: -0.169, loss_o: -0.249\n",
      "Episode: 94, total steps: 1878, episode steps: 43, reward: 43.0, loss_i: -0.575, loss_h: -0.173, loss_o: -0.252\n",
      "Episode: 95, total steps: 1894, episode steps: 16, reward: 16.0, loss_i: -0.568, loss_h: -0.179, loss_o: -0.253\n",
      "Episode: 96, total steps: 1926, episode steps: 32, reward: 32.0, loss_i: -0.575, loss_h: -0.18, loss_o: -0.256\n",
      "Episode: 97, total steps: 1941, episode steps: 15, reward: 15.0, loss_i: -0.567, loss_h: -0.183, loss_o: -0.254\n",
      "Episode: 98, total steps: 1962, episode steps: 21, reward: 21.0, loss_i: -0.573, loss_h: -0.186, loss_o: -0.257\n",
      "Episode: 99, total steps: 1980, episode steps: 18, reward: 18.0, loss_i: -0.564, loss_h: -0.187, loss_o: -0.26\n",
      "Episode: 100, total steps: 1989, episode steps: 9, reward: 9.0, loss_i: -0.549, loss_h: -0.189, loss_o: -0.261\n",
      "INPUT: {'critic_1_loss': 0.031861145230424076, 'critic_2_loss': 0.03957247961726454, 'policy_loss': -1.7181919647587671, 'ent_loss': -2.3384637443555727, 'alpha': 0.01464333832781348, 'cnt': 1.0}\n",
      "HIDDEN: {'critic_1_loss': 0.0036005614173417497, 'critic_2_loss': 0.006620192683638177, 'policy_loss': -0.5772465363972716, 'ent_loss': -2.861074619823032, 'alpha': 0.12741220432023206, 'cnt': 1.0}\n",
      "OUTPUT: {'critic_1_loss': 0.010152650893562369, 'critic_2_loss': 0.010666224588122632, 'policy_loss': -0.8027066787083944, 'ent_loss': -0.7032617396778531, 'alpha': 0.003863833715311355, 'cnt': 1.0}\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 13.6\n",
      "----------------------------------------\n",
      "Episode: 101, total steps: 2000, episode steps: 11, reward: 11.0, loss_i: -0.555, loss_h: -0.19, loss_o: -0.26\n",
      "Episode: 102, total steps: 2010, episode steps: 10, reward: 10.0, loss_i: -0.557, loss_h: -0.191, loss_o: -0.259\n",
      "Episode: 103, total steps: 2023, episode steps: 13, reward: 13.0, loss_i: -0.546, loss_h: -0.192, loss_o: -0.261\n",
      "Episode: 104, total steps: 2057, episode steps: 34, reward: 34.0, loss_i: -0.514, loss_h: -0.194, loss_o: -0.261\n",
      "Episode: 105, total steps: 2076, episode steps: 19, reward: 19.0, loss_i: -0.504, loss_h: -0.195, loss_o: -0.261\n",
      "Episode: 106, total steps: 2097, episode steps: 21, reward: 21.0, loss_i: -0.488, loss_h: -0.196, loss_o: -0.262\n",
      "Episode: 107, total steps: 2112, episode steps: 15, reward: 15.0, loss_i: -0.475, loss_h: -0.198, loss_o: -0.264\n",
      "Episode: 108, total steps: 2126, episode steps: 14, reward: 14.0, loss_i: -0.47, loss_h: -0.199, loss_o: -0.263\n",
      "Episode: 109, total steps: 2137, episode steps: 11, reward: 11.0, loss_i: -0.492, loss_h: -0.2, loss_o: -0.267\n",
      "Episode: 110, total steps: 2147, episode steps: 10, reward: 10.0, loss_i: -0.535, loss_h: -0.202, loss_o: -0.265\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 15.4\n",
      "----------------------------------------\n",
      "Episode: 111, total steps: 2165, episode steps: 18, reward: 18.0, loss_i: -0.543, loss_h: -0.202, loss_o: -0.265\n",
      "Episode: 112, total steps: 2182, episode steps: 17, reward: 17.0, loss_i: -0.533, loss_h: -0.203, loss_o: -0.268\n",
      "Episode: 113, total steps: 2193, episode steps: 11, reward: 11.0, loss_i: -0.537, loss_h: -0.204, loss_o: -0.265\n",
      "Episode: 114, total steps: 2206, episode steps: 13, reward: 13.0, loss_i: -0.538, loss_h: -0.206, loss_o: -0.266\n",
      "Episode: 115, total steps: 2217, episode steps: 11, reward: 11.0, loss_i: -0.54, loss_h: -0.205, loss_o: -0.267\n",
      "Episode: 116, total steps: 2266, episode steps: 49, reward: 49.0, loss_i: -0.539, loss_h: -0.207, loss_o: -0.27\n",
      "Episode: 117, total steps: 2282, episode steps: 16, reward: 16.0, loss_i: -0.542, loss_h: -0.208, loss_o: -0.272\n",
      "Episode: 118, total steps: 2294, episode steps: 12, reward: 12.0, loss_i: -0.55, loss_h: -0.209, loss_o: -0.272\n",
      "Episode: 119, total steps: 2311, episode steps: 17, reward: 17.0, loss_i: -0.546, loss_h: -0.209, loss_o: -0.271\n",
      "Episode: 120, total steps: 2320, episode steps: 9, reward: 9.0, loss_i: -0.538, loss_h: -0.21, loss_o: -0.275\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 15.9\n",
      "----------------------------------------\n",
      "Episode: 121, total steps: 2337, episode steps: 17, reward: 17.0, loss_i: -0.55, loss_h: -0.211, loss_o: -0.274\n",
      "Episode: 122, total steps: 2347, episode steps: 10, reward: 10.0, loss_i: -0.545, loss_h: -0.212, loss_o: -0.277\n",
      "Episode: 123, total steps: 2357, episode steps: 10, reward: 10.0, loss_i: -0.551, loss_h: -0.212, loss_o: -0.272\n",
      "Episode: 124, total steps: 2367, episode steps: 10, reward: 10.0, loss_i: -0.552, loss_h: -0.213, loss_o: -0.274\n",
      "Episode: 125, total steps: 2401, episode steps: 34, reward: 34.0, loss_i: -0.55, loss_h: -0.214, loss_o: -0.278\n",
      "Episode: 126, total steps: 2412, episode steps: 11, reward: 11.0, loss_i: -0.536, loss_h: -0.216, loss_o: -0.279\n",
      "Episode: 127, total steps: 2448, episode steps: 36, reward: 36.0, loss_i: -0.558, loss_h: -0.216, loss_o: -0.279\n",
      "Episode: 128, total steps: 2462, episode steps: 14, reward: 14.0, loss_i: -0.547, loss_h: -0.218, loss_o: -0.282\n",
      "Episode: 129, total steps: 2483, episode steps: 21, reward: 21.0, loss_i: -0.539, loss_h: -0.219, loss_o: -0.282\n",
      "Episode: 130, total steps: 2495, episode steps: 12, reward: 12.0, loss_i: -0.54, loss_h: -0.219, loss_o: -0.28\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 23.2\n",
      "----------------------------------------\n",
      "Episode: 131, total steps: 2516, episode steps: 21, reward: 21.0, loss_i: -0.531, loss_h: -0.22, loss_o: -0.282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 132, total steps: 2525, episode steps: 9, reward: 9.0, loss_i: -0.549, loss_h: -0.221, loss_o: -0.278\n",
      "Episode: 133, total steps: 2539, episode steps: 14, reward: 14.0, loss_i: -0.531, loss_h: -0.221, loss_o: -0.282\n",
      "Episode: 134, total steps: 2551, episode steps: 12, reward: 12.0, loss_i: -0.541, loss_h: -0.222, loss_o: -0.284\n",
      "Episode: 135, total steps: 2562, episode steps: 11, reward: 11.0, loss_i: -0.538, loss_h: -0.223, loss_o: -0.285\n",
      "Episode: 136, total steps: 2574, episode steps: 12, reward: 12.0, loss_i: -0.555, loss_h: -0.223, loss_o: -0.284\n",
      "Episode: 137, total steps: 2591, episode steps: 17, reward: 17.0, loss_i: -0.516, loss_h: -0.224, loss_o: -0.286\n",
      "Episode: 138, total steps: 2607, episode steps: 16, reward: 16.0, loss_i: -0.528, loss_h: -0.225, loss_o: -0.288\n",
      "Episode: 139, total steps: 2619, episode steps: 12, reward: 12.0, loss_i: -0.514, loss_h: -0.225, loss_o: -0.287\n",
      "Episode: 140, total steps: 2629, episode steps: 10, reward: 10.0, loss_i: -0.528, loss_h: -0.226, loss_o: -0.289\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.0\n",
      "----------------------------------------\n",
      "Episode: 141, total steps: 2643, episode steps: 14, reward: 14.0, loss_i: -0.531, loss_h: -0.227, loss_o: -0.29\n",
      "Episode: 142, total steps: 2652, episode steps: 9, reward: 9.0, loss_i: -0.547, loss_h: -0.227, loss_o: -0.293\n",
      "Episode: 143, total steps: 2662, episode steps: 10, reward: 10.0, loss_i: -0.526, loss_h: -0.227, loss_o: -0.29\n",
      "Episode: 144, total steps: 2686, episode steps: 24, reward: 24.0, loss_i: -0.532, loss_h: -0.228, loss_o: -0.29\n",
      "Episode: 145, total steps: 2705, episode steps: 19, reward: 19.0, loss_i: -0.534, loss_h: -0.23, loss_o: -0.295\n",
      "Episode: 146, total steps: 2718, episode steps: 13, reward: 13.0, loss_i: -0.535, loss_h: -0.23, loss_o: -0.292\n",
      "Episode: 147, total steps: 2762, episode steps: 44, reward: 44.0, loss_i: -0.548, loss_h: -0.231, loss_o: -0.297\n",
      "Episode: 148, total steps: 2773, episode steps: 11, reward: 11.0, loss_i: -0.546, loss_h: -0.233, loss_o: -0.296\n",
      "Episode: 149, total steps: 2784, episode steps: 11, reward: 11.0, loss_i: -0.537, loss_h: -0.234, loss_o: -0.294\n",
      "Episode: 150, total steps: 2798, episode steps: 14, reward: 14.0, loss_i: -0.542, loss_h: -0.233, loss_o: -0.294\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 14.6\n",
      "----------------------------------------\n",
      "Episode: 151, total steps: 2809, episode steps: 11, reward: 11.0, loss_i: -0.55, loss_h: -0.235, loss_o: -0.295\n",
      "Episode: 152, total steps: 2819, episode steps: 10, reward: 10.0, loss_i: -0.554, loss_h: -0.235, loss_o: -0.296\n",
      "Episode: 153, total steps: 2854, episode steps: 35, reward: 35.0, loss_i: -0.544, loss_h: -0.236, loss_o: -0.298\n",
      "Episode: 154, total steps: 2865, episode steps: 11, reward: 11.0, loss_i: -0.532, loss_h: -0.237, loss_o: -0.3\n",
      "Episode: 155, total steps: 2885, episode steps: 20, reward: 20.0, loss_i: -0.565, loss_h: -0.238, loss_o: -0.297\n",
      "Episode: 156, total steps: 2896, episode steps: 11, reward: 11.0, loss_i: -0.568, loss_h: -0.239, loss_o: -0.304\n",
      "Episode: 157, total steps: 2910, episode steps: 14, reward: 14.0, loss_i: -0.544, loss_h: -0.239, loss_o: -0.3\n",
      "Episode: 158, total steps: 2923, episode steps: 13, reward: 13.0, loss_i: -0.575, loss_h: -0.24, loss_o: -0.303\n",
      "Episode: 159, total steps: 2933, episode steps: 10, reward: 10.0, loss_i: -0.602, loss_h: -0.241, loss_o: -0.298\n",
      "Episode: 160, total steps: 2945, episode steps: 12, reward: 12.0, loss_i: -0.598, loss_h: -0.241, loss_o: -0.3\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 13.1\n",
      "----------------------------------------\n",
      "Episode: 161, total steps: 2955, episode steps: 10, reward: 10.0, loss_i: -0.585, loss_h: -0.242, loss_o: -0.306\n",
      "Episode: 162, total steps: 2966, episode steps: 11, reward: 11.0, loss_i: -0.584, loss_h: -0.242, loss_o: -0.309\n",
      "Episode: 163, total steps: 2985, episode steps: 19, reward: 19.0, loss_i: -0.567, loss_h: -0.243, loss_o: -0.306\n",
      "Episode: 164, total steps: 2999, episode steps: 14, reward: 14.0, loss_i: -0.569, loss_h: -0.244, loss_o: -0.306\n",
      "Episode: 165, total steps: 3018, episode steps: 19, reward: 19.0, loss_i: -0.56, loss_h: -0.245, loss_o: -0.305\n",
      "Episode: 166, total steps: 3031, episode steps: 13, reward: 13.0, loss_i: -0.555, loss_h: -0.244, loss_o: -0.302\n",
      "Episode: 167, total steps: 3043, episode steps: 12, reward: 12.0, loss_i: -0.543, loss_h: -0.247, loss_o: -0.307\n",
      "Episode: 168, total steps: 3057, episode steps: 14, reward: 14.0, loss_i: -0.539, loss_h: -0.246, loss_o: -0.309\n",
      "Episode: 169, total steps: 3066, episode steps: 9, reward: 9.0, loss_i: -0.534, loss_h: -0.247, loss_o: -0.306\n",
      "Episode: 170, total steps: 3078, episode steps: 12, reward: 12.0, loss_i: -0.525, loss_h: -0.248, loss_o: -0.31\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 11.7\n",
      "----------------------------------------\n",
      "Episode: 171, total steps: 3089, episode steps: 11, reward: 11.0, loss_i: -0.529, loss_h: -0.25, loss_o: -0.31\n",
      "Episode: 172, total steps: 3098, episode steps: 9, reward: 9.0, loss_i: -0.535, loss_h: -0.249, loss_o: -0.312\n",
      "Episode: 173, total steps: 3112, episode steps: 14, reward: 14.0, loss_i: -0.521, loss_h: -0.249, loss_o: -0.313\n",
      "Episode: 174, total steps: 3128, episode steps: 16, reward: 16.0, loss_i: -0.509, loss_h: -0.251, loss_o: -0.314\n",
      "Episode: 175, total steps: 3138, episode steps: 10, reward: 10.0, loss_i: -0.514, loss_h: -0.251, loss_o: -0.312\n",
      "Episode: 176, total steps: 3156, episode steps: 18, reward: 18.0, loss_i: -0.514, loss_h: -0.251, loss_o: -0.312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1625cdee08be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mhidden_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     \u001b[0mcritic_1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mhidden_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_1_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcritic_1_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mhidden_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcritic_2_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/sac.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mmin_qf_next_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqf1_next_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqf2_next_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_state_log_pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mnext_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin_qf_next_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mqf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Two Q-functions to mitigate positive bias in the policy improvement step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mqf1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_q_value\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mqf2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_q_value\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/cascade_rl/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "updates = 0\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    reasoned_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    inner_activations = {}\n",
    "    action = None\n",
    "    train_stats = Dict()\n",
    "    train_stats.input = []\n",
    "    train_stats.hidden = []\n",
    "    train_stats.output = []\n",
    "    while not done:\n",
    "        if WARM_UP > total_steps:\n",
    "            action, inner_activations = sample_layers(state)  # Sample random action\n",
    "        else:\n",
    "            if not inner_activations:\n",
    "                action, inner_activations = eval_layers(state)  # Sample action from policy\n",
    "            if inner_activations['output_state_actions'][0]:\n",
    "                reasoned_steps += 1\n",
    "\n",
    "        # Number of updates per step in environment\n",
    "        for i in range(UPDATES_PER_STEP):\n",
    "            # Update parameters of all the networks\n",
    "            flag = False\n",
    "            for agent, memory in zip(input_layer, input_memory):\n",
    "                input_stats = Dict()\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    input_stats.critic_1_loss += critic_1_loss\n",
    "                    input_stats.critic_2_loss += critic_2_loss\n",
    "                    input_stats.policy_loss += policy_loss\n",
    "                    input_stats.ent_loss += ent_loss\n",
    "                    input_stats.alpha += alpha\n",
    "                    input_stats.cnt += 1\n",
    "                    flag = True\n",
    "                norm_stats(input_stats)\n",
    "                train_stats.input.append(input_stats)\n",
    "            for agent, memory in zip(hidden_layer, hidden_memory):\n",
    "                hidden_stats = Dict()\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    hidden_stats.critic_1_loss += critic_1_loss\n",
    "                    hidden_stats.critic_2_loss += critic_2_loss\n",
    "                    hidden_stats.policy_loss += policy_loss\n",
    "                    hidden_stats.ent_loss += ent_loss\n",
    "                    hidden_stats.alpha += alpha\n",
    "                    hidden_stats.cnt += 1\n",
    "                    flag = True\n",
    "                norm_stats(hidden_stats)\n",
    "                train_stats.hidden.append(hidden_stats)\n",
    "            for agent, memory in zip(output_layer, output_memory):\n",
    "                output_stats = Dict()\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "                    output_stats.critic_1_loss += critic_1_loss\n",
    "                    output_stats.critic_2_loss += critic_2_loss\n",
    "                    output_stats.policy_loss += policy_loss\n",
    "                    output_stats.ent_loss += ent_loss\n",
    "                    output_stats.alpha += alpha\n",
    "                    output_stats.cnt += 1\n",
    "                    flag = True\n",
    "                norm_stats(output_stats)\n",
    "                train_stats.output.append(output_stats)\n",
    "            updates += flag\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "        action, inner_activations = push_memory(inner_activations, reward, next_state, mask)\n",
    "\n",
    "        state = next_state\n",
    "    train_stats.input = average_stats(train_stats.input)\n",
    "    train_stats.hidden = average_stats(train_stats.hidden)\n",
    "    train_stats.output = average_stats(train_stats.output)\n",
    "    loss = get_avg_loss(train_stats)\n",
    "    print(\"Episode: {}, total steps: {}, episode steps: {}, reward: {}, loss_i: {}, loss_h: {}, loss_o: {}\".format(i_episode, total_steps, episode_steps, round(episode_reward, 2), *list(map(lambda x : round(x, 3), loss))))\n",
    "    if updates > 0 and i_episode % PRINT_FREQ == 0:\n",
    "        print('INPUT: %s' % str(train_stats.input))\n",
    "        print('HIDDEN: %s' % str(train_stats.hidden))\n",
    "        print('OUTPUT: %s' % str(train_stats.output))\n",
    "    \n",
    "    if i_episode % 10 == 0:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = eval_layers(state)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
