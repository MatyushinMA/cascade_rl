{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sac import SAC\n",
    "from replay_memory import ReplayMemory\n",
    "import gym\n",
    "from addict import Dict\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def norm_stats(stats):\n",
    "    for k in stats:\n",
    "        stats[k] /= stats.cnt\n",
    "    return stats\n",
    "\n",
    "def average_stats(lst):\n",
    "    avg = Dict()\n",
    "    for stats in lst:\n",
    "        for k in stats:\n",
    "            avg[k] += stats[k]\n",
    "        avg.cnt += 1\n",
    "    return norm_stats(avg)\n",
    "\n",
    "def get_avg_loss(train_stats):\n",
    "    try:\n",
    "        return (train_stats.critic_1_loss + train_stats.critic_2_loss + train_stats.policy_loss)/3\n",
    "    except:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP = 10000\n",
    "BUFFER_SIZE = 10**6\n",
    "BATCH_SIZE = 256\n",
    "UPDATES_PER_STEP = 1\n",
    "PRINT_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space.shape = (1,)\n",
    "env.action_space.high = np.array([1])\n",
    "env.action_space.low = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dict()\n",
    "args.gamma = 0.99\n",
    "args.tau = 1\n",
    "args.alpha = 0.2\n",
    "args.policy = 'Gaussian'\n",
    "args.target_update_interval = 1000\n",
    "args.automatic_entropy_tuning = False\n",
    "args.cuda = False\n",
    "args.hidden_size = 256\n",
    "args.lr = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(4, env.action_space, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, total steps: 31, episode steps: 31, reward: 31.0, loss: nan\n",
      "Episode: 2, total steps: 48, episode steps: 17, reward: 17.0, loss: nan\n",
      "Episode: 3, total steps: 68, episode steps: 20, reward: 20.0, loss: nan\n",
      "Episode: 4, total steps: 146, episode steps: 78, reward: 78.0, loss: nan\n",
      "Episode: 5, total steps: 163, episode steps: 17, reward: 17.0, loss: nan\n",
      "Episode: 6, total steps: 216, episode steps: 53, reward: 53.0, loss: nan\n",
      "Episode: 7, total steps: 254, episode steps: 38, reward: 38.0, loss: nan\n",
      "Episode: 8, total steps: 280, episode steps: 26, reward: 26.0, loss: 0.415\n",
      "Episode: 9, total steps: 312, episode steps: 32, reward: 32.0, loss: 0.035\n",
      "Episode: 10, total steps: 334, episode steps: 22, reward: 22.0, loss: -0.134\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.0, Max. Reward: 45.0\n",
      "----------------------------------------\n",
      "Episode: 11, total steps: 380, episode steps: 46, reward: 46.0, loss: -0.218\n",
      "Episode: 12, total steps: 406, episode steps: 26, reward: 26.0, loss: -0.257\n",
      "Episode: 13, total steps: 426, episode steps: 20, reward: 20.0, loss: -0.269\n",
      "Episode: 14, total steps: 449, episode steps: 23, reward: 23.0, loss: -0.276\n",
      "Episode: 15, total steps: 466, episode steps: 17, reward: 17.0, loss: -0.281\n",
      "Episode: 16, total steps: 483, episode steps: 17, reward: 17.0, loss: -0.284\n",
      "Episode: 17, total steps: 511, episode steps: 28, reward: 28.0, loss: -0.287\n",
      "Episode: 18, total steps: 519, episode steps: 8, reward: 8.0, loss: -0.29\n",
      "Episode: 19, total steps: 549, episode steps: 30, reward: 30.0, loss: -0.291\n",
      "Episode: 20, total steps: 572, episode steps: 23, reward: 23.0, loss: -0.293\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 18.7, Max. Reward: 32.0\n",
      "----------------------------------------\n",
      "Episode: 21, total steps: 586, episode steps: 14, reward: 14.0, loss: -0.294\n",
      "Episode: 22, total steps: 608, episode steps: 22, reward: 22.0, loss: -0.295\n",
      "Episode: 23, total steps: 652, episode steps: 44, reward: 44.0, loss: -0.296\n",
      "Episode: 24, total steps: 679, episode steps: 27, reward: 27.0, loss: -0.297\n",
      "Episode: 25, total steps: 692, episode steps: 13, reward: 13.0, loss: -0.298\n",
      "Episode: 26, total steps: 703, episode steps: 11, reward: 11.0, loss: -0.299\n",
      "Episode: 27, total steps: 719, episode steps: 16, reward: 16.0, loss: -0.3\n",
      "Episode: 28, total steps: 749, episode steps: 30, reward: 30.0, loss: -0.301\n",
      "Episode: 29, total steps: 762, episode steps: 13, reward: 13.0, loss: -0.302\n",
      "Episode: 30, total steps: 776, episode steps: 14, reward: 14.0, loss: -0.303\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 24.1, Max. Reward: 35.0\n",
      "----------------------------------------\n",
      "Episode: 31, total steps: 801, episode steps: 25, reward: 25.0, loss: -0.303\n",
      "Episode: 32, total steps: 819, episode steps: 18, reward: 18.0, loss: -0.303\n",
      "Episode: 33, total steps: 842, episode steps: 23, reward: 23.0, loss: -0.304\n",
      "Episode: 34, total steps: 859, episode steps: 17, reward: 17.0, loss: -0.305\n",
      "Episode: 35, total steps: 888, episode steps: 29, reward: 29.0, loss: -0.305\n",
      "Episode: 36, total steps: 898, episode steps: 10, reward: 10.0, loss: -0.306\n",
      "Episode: 37, total steps: 916, episode steps: 18, reward: 18.0, loss: -0.305\n",
      "Episode: 38, total steps: 934, episode steps: 18, reward: 18.0, loss: -0.306\n",
      "Episode: 39, total steps: 948, episode steps: 14, reward: 14.0, loss: -0.306\n",
      "Episode: 40, total steps: 984, episode steps: 36, reward: 36.0, loss: -0.307\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 19.1, Max. Reward: 30.0\n",
      "----------------------------------------\n",
      "Episode: 41, total steps: 1001, episode steps: 17, reward: 17.0, loss: -0.307\n",
      "Episode: 42, total steps: 1014, episode steps: 13, reward: 13.0, loss: -0.307\n",
      "Episode: 43, total steps: 1044, episode steps: 30, reward: 30.0, loss: -0.307\n",
      "Episode: 44, total steps: 1056, episode steps: 12, reward: 12.0, loss: -0.308\n",
      "Episode: 45, total steps: 1071, episode steps: 15, reward: 15.0, loss: -0.308\n",
      "Episode: 46, total steps: 1093, episode steps: 22, reward: 22.0, loss: -0.308\n",
      "Episode: 47, total steps: 1107, episode steps: 14, reward: 14.0, loss: -0.308\n",
      "Episode: 48, total steps: 1117, episode steps: 10, reward: 10.0, loss: -0.308\n",
      "Episode: 49, total steps: 1130, episode steps: 13, reward: 13.0, loss: -0.308\n",
      "Episode: 50, total steps: 1146, episode steps: 16, reward: 16.0, loss: -0.308\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.1, Max. Reward: 45.0\n",
      "----------------------------------------\n",
      "Episode: 51, total steps: 1162, episode steps: 16, reward: 16.0, loss: -0.308\n",
      "Episode: 52, total steps: 1197, episode steps: 35, reward: 35.0, loss: -0.309\n",
      "Episode: 53, total steps: 1210, episode steps: 13, reward: 13.0, loss: -0.309\n",
      "Episode: 54, total steps: 1240, episode steps: 30, reward: 30.0, loss: -0.309\n",
      "Episode: 55, total steps: 1251, episode steps: 11, reward: 11.0, loss: -0.309\n",
      "Episode: 56, total steps: 1261, episode steps: 10, reward: 10.0, loss: -0.137\n",
      "Episode: 57, total steps: 1292, episode steps: 31, reward: 31.0, loss: -0.142\n",
      "Episode: 58, total steps: 1303, episode steps: 11, reward: 11.0, loss: -0.408\n",
      "Episode: 59, total steps: 1319, episode steps: 16, reward: 16.0, loss: -0.477\n",
      "Episode: 60, total steps: 1342, episode steps: 23, reward: 23.0, loss: -0.525\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 22.5, Max. Reward: 37.0\n",
      "----------------------------------------\n",
      "Episode: 61, total steps: 1354, episode steps: 12, reward: 12.0, loss: -0.545\n",
      "Episode: 62, total steps: 1368, episode steps: 14, reward: 14.0, loss: -0.557\n",
      "Episode: 63, total steps: 1426, episode steps: 58, reward: 58.0, loss: -0.563\n",
      "Episode: 64, total steps: 1440, episode steps: 14, reward: 14.0, loss: -0.568\n",
      "Episode: 65, total steps: 1459, episode steps: 19, reward: 19.0, loss: -0.57\n",
      "Episode: 66, total steps: 1482, episode steps: 23, reward: 23.0, loss: -0.575\n",
      "Episode: 67, total steps: 1495, episode steps: 13, reward: 13.0, loss: -0.579\n",
      "Episode: 68, total steps: 1506, episode steps: 11, reward: 11.0, loss: -0.573\n",
      "Episode: 69, total steps: 1549, episode steps: 43, reward: 43.0, loss: -0.578\n",
      "Episode: 70, total steps: 1564, episode steps: 15, reward: 15.0, loss: -0.583\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 19.3, Max. Reward: 45.0\n",
      "----------------------------------------\n",
      "Episode: 71, total steps: 1608, episode steps: 44, reward: 44.0, loss: -0.582\n",
      "Episode: 72, total steps: 1651, episode steps: 43, reward: 43.0, loss: -0.585\n",
      "Episode: 73, total steps: 1662, episode steps: 11, reward: 11.0, loss: -0.584\n",
      "Episode: 74, total steps: 1675, episode steps: 13, reward: 13.0, loss: -0.588\n",
      "Episode: 75, total steps: 1688, episode steps: 13, reward: 13.0, loss: -0.587\n",
      "Episode: 76, total steps: 1709, episode steps: 21, reward: 21.0, loss: -0.587\n",
      "Episode: 77, total steps: 1718, episode steps: 9, reward: 9.0, loss: -0.582\n",
      "Episode: 78, total steps: 1762, episode steps: 44, reward: 44.0, loss: -0.587\n",
      "Episode: 79, total steps: 1774, episode steps: 12, reward: 12.0, loss: -0.587\n",
      "Episode: 80, total steps: 1788, episode steps: 14, reward: 14.0, loss: -0.59\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 20.0, Max. Reward: 41.0\n",
      "----------------------------------------\n",
      "Episode: 81, total steps: 1798, episode steps: 10, reward: 10.0, loss: -0.586\n",
      "Episode: 82, total steps: 1815, episode steps: 17, reward: 17.0, loss: -0.591\n",
      "Episode: 83, total steps: 1843, episode steps: 28, reward: 28.0, loss: -0.591\n",
      "Episode: 84, total steps: 1858, episode steps: 15, reward: 15.0, loss: -0.591\n",
      "Episode: 85, total steps: 1874, episode steps: 16, reward: 16.0, loss: -0.591\n",
      "Episode: 86, total steps: 1889, episode steps: 15, reward: 15.0, loss: -0.592\n",
      "Episode: 87, total steps: 1914, episode steps: 25, reward: 25.0, loss: -0.59\n",
      "Episode: 88, total steps: 1929, episode steps: 15, reward: 15.0, loss: -0.59\n",
      "Episode: 89, total steps: 1946, episode steps: 17, reward: 17.0, loss: -0.593\n",
      "Episode: 90, total steps: 1977, episode steps: 31, reward: 31.0, loss: -0.591\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 20.7, Max. Reward: 46.0\n",
      "----------------------------------------\n",
      "Episode: 91, total steps: 2001, episode steps: 24, reward: 24.0, loss: -0.593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 92, total steps: 2045, episode steps: 44, reward: 44.0, loss: -0.596\n",
      "Episode: 93, total steps: 2058, episode steps: 13, reward: 13.0, loss: -0.594\n",
      "Episode: 94, total steps: 2066, episode steps: 8, reward: 8.0, loss: -0.595\n",
      "Episode: 95, total steps: 2079, episode steps: 13, reward: 13.0, loss: -0.595\n",
      "Episode: 96, total steps: 2096, episode steps: 17, reward: 17.0, loss: -0.594\n",
      "Episode: 97, total steps: 2106, episode steps: 10, reward: 10.0, loss: -0.594\n",
      "Episode: 98, total steps: 2135, episode steps: 29, reward: 29.0, loss: -0.592\n",
      "Episode: 99, total steps: 2151, episode steps: 16, reward: 16.0, loss: -0.591\n",
      "Episode: 100, total steps: 2175, episode steps: 24, reward: 24.0, loss: -0.593\n",
      "TRAIN STATS: {'critic_1_loss': 0.03792622988112271, 'critic_2_loss': 0.03847720636986196, 'policy_loss': -1.8542451808849971, 'ent_loss': 0.0, 'alpha': 0.20000000298023224, 'cnt': 1.0}\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 17.5, Max. Reward: 37.0\n",
      "----------------------------------------\n",
      "Episode: 101, total steps: 2205, episode steps: 30, reward: 30.0, loss: -0.597\n",
      "Episode: 102, total steps: 2220, episode steps: 15, reward: 15.0, loss: -0.594\n",
      "Episode: 103, total steps: 2248, episode steps: 28, reward: 28.0, loss: -0.594\n",
      "Episode: 104, total steps: 2259, episode steps: 11, reward: 11.0, loss: -0.54\n",
      "Episode: 105, total steps: 2282, episode steps: 23, reward: 23.0, loss: -0.419\n",
      "Episode: 106, total steps: 2299, episode steps: 17, reward: 17.0, loss: -0.701\n",
      "Episode: 107, total steps: 2321, episode steps: 22, reward: 22.0, loss: -0.765\n",
      "Episode: 108, total steps: 2353, episode steps: 32, reward: 32.0, loss: -0.782\n",
      "Episode: 109, total steps: 2367, episode steps: 14, reward: 14.0, loss: -0.806\n",
      "Episode: 110, total steps: 2389, episode steps: 22, reward: 22.0, loss: -0.799\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 17.7, Max. Reward: 25.0\n",
      "----------------------------------------\n",
      "Episode: 111, total steps: 2404, episode steps: 15, reward: 15.0, loss: -0.808\n",
      "Episode: 112, total steps: 2441, episode steps: 37, reward: 37.0, loss: -0.806\n",
      "Episode: 113, total steps: 2457, episode steps: 16, reward: 16.0, loss: -0.804\n",
      "Episode: 114, total steps: 2521, episode steps: 64, reward: 64.0, loss: -0.812\n",
      "Episode: 115, total steps: 2561, episode steps: 40, reward: 40.0, loss: -0.809\n",
      "Episode: 116, total steps: 2584, episode steps: 23, reward: 23.0, loss: -0.817\n",
      "Episode: 117, total steps: 2606, episode steps: 22, reward: 22.0, loss: -0.827\n",
      "Episode: 118, total steps: 2621, episode steps: 15, reward: 15.0, loss: -0.822\n",
      "Episode: 119, total steps: 2640, episode steps: 19, reward: 19.0, loss: -0.82\n",
      "Episode: 120, total steps: 2661, episode steps: 21, reward: 21.0, loss: -0.819\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 22.3, Max. Reward: 49.0\n",
      "----------------------------------------\n",
      "Episode: 121, total steps: 2675, episode steps: 14, reward: 14.0, loss: -0.816\n",
      "Episode: 122, total steps: 2703, episode steps: 28, reward: 28.0, loss: -0.827\n",
      "Episode: 123, total steps: 2722, episode steps: 19, reward: 19.0, loss: -0.824\n",
      "Episode: 124, total steps: 2772, episode steps: 50, reward: 50.0, loss: -0.819\n",
      "Episode: 125, total steps: 2790, episode steps: 18, reward: 18.0, loss: -0.818\n",
      "Episode: 126, total steps: 2807, episode steps: 17, reward: 17.0, loss: -0.822\n",
      "Episode: 127, total steps: 2822, episode steps: 15, reward: 15.0, loss: -0.833\n",
      "Episode: 128, total steps: 2842, episode steps: 20, reward: 20.0, loss: -0.823\n",
      "Episode: 129, total steps: 2881, episode steps: 39, reward: 39.0, loss: -0.821\n",
      "Episode: 130, total steps: 2903, episode steps: 22, reward: 22.0, loss: -0.83\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 23.7, Max. Reward: 50.0\n",
      "----------------------------------------\n",
      "Episode: 131, total steps: 2912, episode steps: 9, reward: 9.0, loss: -0.83\n",
      "Episode: 132, total steps: 2928, episode steps: 16, reward: 16.0, loss: -0.831\n",
      "Episode: 133, total steps: 2940, episode steps: 12, reward: 12.0, loss: -0.825\n",
      "Episode: 134, total steps: 2986, episode steps: 46, reward: 46.0, loss: -0.825\n",
      "Episode: 135, total steps: 3008, episode steps: 22, reward: 22.0, loss: -0.833\n",
      "Episode: 136, total steps: 3023, episode steps: 15, reward: 15.0, loss: -0.826\n",
      "Episode: 137, total steps: 3037, episode steps: 14, reward: 14.0, loss: -0.818\n",
      "Episode: 138, total steps: 3078, episode steps: 41, reward: 41.0, loss: -0.829\n",
      "Episode: 139, total steps: 3092, episode steps: 14, reward: 14.0, loss: -0.829\n",
      "Episode: 140, total steps: 3105, episode steps: 13, reward: 13.0, loss: -0.83\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 18.0, Max. Reward: 39.0\n",
      "----------------------------------------\n",
      "Episode: 141, total steps: 3125, episode steps: 20, reward: 20.0, loss: -0.826\n",
      "Episode: 142, total steps: 3158, episode steps: 33, reward: 33.0, loss: -0.834\n",
      "Episode: 143, total steps: 3210, episode steps: 52, reward: 52.0, loss: -0.832\n",
      "Episode: 144, total steps: 3234, episode steps: 24, reward: 24.0, loss: -0.826\n",
      "Episode: 145, total steps: 3251, episode steps: 17, reward: 17.0, loss: -0.827\n",
      "Episode: 146, total steps: 3304, episode steps: 53, reward: 53.0, loss: -0.832\n",
      "Episode: 147, total steps: 3337, episode steps: 33, reward: 33.0, loss: -0.996\n",
      "Episode: 148, total steps: 3363, episode steps: 26, reward: 26.0, loss: -1.0\n",
      "Episode: 149, total steps: 3403, episode steps: 40, reward: 40.0, loss: -1.027\n",
      "Episode: 150, total steps: 3422, episode steps: 19, reward: 19.0, loss: -1.019\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 16.3, Max. Reward: 26.0\n",
      "----------------------------------------\n",
      "Episode: 151, total steps: 3440, episode steps: 18, reward: 18.0, loss: -1.001\n",
      "Episode: 152, total steps: 3470, episode steps: 30, reward: 30.0, loss: -1.016\n",
      "Episode: 153, total steps: 3505, episode steps: 35, reward: 35.0, loss: -1.033\n",
      "Episode: 154, total steps: 3525, episode steps: 20, reward: 20.0, loss: -1.031\n",
      "Episode: 155, total steps: 3548, episode steps: 23, reward: 23.0, loss: -1.027\n",
      "Episode: 156, total steps: 3569, episode steps: 21, reward: 21.0, loss: -1.024\n",
      "Episode: 157, total steps: 3587, episode steps: 18, reward: 18.0, loss: -1.039\n",
      "Episode: 158, total steps: 3620, episode steps: 33, reward: 33.0, loss: -1.021\n",
      "Episode: 159, total steps: 3630, episode steps: 10, reward: 10.0, loss: -1.01\n",
      "Episode: 160, total steps: 3642, episode steps: 12, reward: 12.0, loss: -1.04\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 19.2, Max. Reward: 30.0\n",
      "----------------------------------------\n",
      "Episode: 161, total steps: 3694, episode steps: 52, reward: 52.0, loss: -1.018\n",
      "Episode: 162, total steps: 3742, episode steps: 48, reward: 48.0, loss: -1.032\n",
      "Episode: 163, total steps: 3754, episode steps: 12, reward: 12.0, loss: -1.037\n",
      "Episode: 164, total steps: 3778, episode steps: 24, reward: 24.0, loss: -1.025\n",
      "Episode: 165, total steps: 3802, episode steps: 24, reward: 24.0, loss: -1.034\n",
      "Episode: 166, total steps: 3813, episode steps: 11, reward: 11.0, loss: -1.05\n",
      "Episode: 167, total steps: 3834, episode steps: 21, reward: 21.0, loss: -1.039\n",
      "Episode: 168, total steps: 3851, episode steps: 17, reward: 17.0, loss: -1.028\n",
      "Episode: 169, total steps: 3867, episode steps: 16, reward: 16.0, loss: -1.042\n",
      "Episode: 170, total steps: 3917, episode steps: 50, reward: 50.0, loss: -1.043\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 25.8, Max. Reward: 53.0\n",
      "----------------------------------------\n",
      "Episode: 171, total steps: 3931, episode steps: 14, reward: 14.0, loss: -1.036\n",
      "Episode: 172, total steps: 3943, episode steps: 12, reward: 12.0, loss: -1.042\n",
      "Episode: 173, total steps: 3969, episode steps: 26, reward: 26.0, loss: -1.028\n",
      "Episode: 174, total steps: 3998, episode steps: 29, reward: 29.0, loss: -1.038\n",
      "Episode: 175, total steps: 4023, episode steps: 25, reward: 25.0, loss: -1.043\n",
      "Episode: 176, total steps: 4040, episode steps: 17, reward: 17.0, loss: -1.021\n",
      "Episode: 177, total steps: 4067, episode steps: 27, reward: 27.0, loss: -1.044\n",
      "Episode: 178, total steps: 4087, episode steps: 20, reward: 20.0, loss: -1.033\n",
      "Episode: 179, total steps: 4104, episode steps: 17, reward: 17.0, loss: -1.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, total steps: 4116, episode steps: 12, reward: 12.0, loss: -1.052\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 24.2, Max. Reward: 43.0\n",
      "----------------------------------------\n",
      "Episode: 181, total steps: 4127, episode steps: 11, reward: 11.0, loss: -1.047\n",
      "Episode: 182, total steps: 4141, episode steps: 14, reward: 14.0, loss: -1.039\n",
      "Episode: 183, total steps: 4154, episode steps: 13, reward: 13.0, loss: -1.023\n",
      "Episode: 184, total steps: 4170, episode steps: 16, reward: 16.0, loss: -1.034\n",
      "Episode: 185, total steps: 4193, episode steps: 23, reward: 23.0, loss: -1.059\n",
      "Episode: 186, total steps: 4204, episode steps: 11, reward: 11.0, loss: -1.018\n",
      "Episode: 187, total steps: 4221, episode steps: 17, reward: 17.0, loss: -1.039\n",
      "Episode: 188, total steps: 4239, episode steps: 18, reward: 18.0, loss: -1.035\n",
      "Episode: 189, total steps: 4281, episode steps: 42, reward: 42.0, loss: -1.002\n",
      "Episode: 190, total steps: 4362, episode steps: 81, reward: 81.0, loss: -1.183\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 21.9, Max. Reward: 36.0\n",
      "----------------------------------------\n",
      "Episode: 191, total steps: 4379, episode steps: 17, reward: 17.0, loss: -1.202\n",
      "Episode: 192, total steps: 4392, episode steps: 13, reward: 13.0, loss: -1.147\n",
      "Episode: 193, total steps: 4419, episode steps: 27, reward: 27.0, loss: -1.196\n",
      "Episode: 194, total steps: 4430, episode steps: 11, reward: 11.0, loss: -1.209\n",
      "Episode: 195, total steps: 4460, episode steps: 30, reward: 30.0, loss: -1.189\n",
      "Episode: 196, total steps: 4478, episode steps: 18, reward: 18.0, loss: -1.235\n",
      "Episode: 197, total steps: 4514, episode steps: 36, reward: 36.0, loss: -1.218\n",
      "Episode: 198, total steps: 4542, episode steps: 28, reward: 28.0, loss: -1.204\n",
      "Episode: 199, total steps: 4554, episode steps: 12, reward: 12.0, loss: -1.228\n",
      "Episode: 200, total steps: 4600, episode steps: 46, reward: 46.0, loss: -1.212\n",
      "TRAIN STATS: {'critic_1_loss': 0.3356944711312004, 'critic_2_loss': 0.3393629348796347, 'policy_loss': -4.312240289605183, 'ent_loss': 0.0, 'alpha': 0.20000000298023224, 'cnt': 1.0}\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 31.0, Max. Reward: 81.0\n",
      "----------------------------------------\n",
      "Episode: 201, total steps: 4616, episode steps: 16, reward: 16.0, loss: -1.206\n",
      "Episode: 202, total steps: 4637, episode steps: 21, reward: 21.0, loss: -1.206\n",
      "Episode: 203, total steps: 4694, episode steps: 57, reward: 57.0, loss: -1.207\n",
      "Episode: 204, total steps: 4705, episode steps: 11, reward: 11.0, loss: -1.203\n",
      "Episode: 205, total steps: 4722, episode steps: 17, reward: 17.0, loss: -1.23\n",
      "Episode: 206, total steps: 4777, episode steps: 55, reward: 55.0, loss: -1.242\n",
      "Episode: 207, total steps: 4791, episode steps: 14, reward: 14.0, loss: -1.253\n",
      "Episode: 208, total steps: 4803, episode steps: 12, reward: 12.0, loss: -1.235\n",
      "Episode: 209, total steps: 4817, episode steps: 14, reward: 14.0, loss: -1.181\n",
      "Episode: 210, total steps: 4836, episode steps: 19, reward: 19.0, loss: -1.229\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 23.6, Max. Reward: 50.0\n",
      "----------------------------------------\n",
      "Episode: 211, total steps: 4853, episode steps: 17, reward: 17.0, loss: -1.237\n",
      "Episode: 212, total steps: 4880, episode steps: 27, reward: 27.0, loss: -1.239\n",
      "Episode: 213, total steps: 4918, episode steps: 38, reward: 38.0, loss: -1.214\n",
      "Episode: 214, total steps: 4938, episode steps: 20, reward: 20.0, loss: -1.228\n",
      "Episode: 215, total steps: 4951, episode steps: 13, reward: 13.0, loss: -1.224\n",
      "Episode: 216, total steps: 4969, episode steps: 18, reward: 18.0, loss: -1.207\n",
      "Episode: 217, total steps: 4990, episode steps: 21, reward: 21.0, loss: -1.222\n",
      "Episode: 218, total steps: 5009, episode steps: 19, reward: 19.0, loss: -1.226\n",
      "Episode: 219, total steps: 5025, episode steps: 16, reward: 16.0, loss: -1.196\n",
      "Episode: 220, total steps: 5051, episode steps: 26, reward: 26.0, loss: -1.228\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 22.3, Max. Reward: 66.0\n",
      "----------------------------------------\n",
      "Episode: 221, total steps: 5068, episode steps: 17, reward: 17.0, loss: -1.196\n",
      "Episode: 222, total steps: 5080, episode steps: 12, reward: 12.0, loss: -1.208\n",
      "Episode: 223, total steps: 5090, episode steps: 10, reward: 10.0, loss: -1.237\n",
      "Episode: 224, total steps: 5104, episode steps: 14, reward: 14.0, loss: -1.241\n",
      "Episode: 225, total steps: 5122, episode steps: 18, reward: 18.0, loss: -1.213\n",
      "Episode: 226, total steps: 5143, episode steps: 21, reward: 21.0, loss: -1.225\n",
      "Episode: 227, total steps: 5185, episode steps: 42, reward: 42.0, loss: -1.235\n",
      "Episode: 228, total steps: 5198, episode steps: 13, reward: 13.0, loss: -1.25\n",
      "Episode: 229, total steps: 5213, episode steps: 15, reward: 15.0, loss: -1.219\n",
      "Episode: 230, total steps: 5261, episode steps: 48, reward: 48.0, loss: -1.208\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 30.7, Max. Reward: 55.0\n",
      "----------------------------------------\n",
      "Episode: 231, total steps: 5279, episode steps: 18, reward: 18.0, loss: -1.254\n",
      "Episode: 232, total steps: 5299, episode steps: 20, reward: 20.0, loss: -1.324\n",
      "Episode: 233, total steps: 5315, episode steps: 16, reward: 16.0, loss: -1.328\n",
      "Episode: 234, total steps: 5341, episode steps: 26, reward: 26.0, loss: -1.338\n",
      "Episode: 235, total steps: 5375, episode steps: 34, reward: 34.0, loss: -1.382\n",
      "Episode: 236, total steps: 5413, episode steps: 38, reward: 38.0, loss: -1.375\n",
      "Episode: 237, total steps: 5436, episode steps: 23, reward: 23.0, loss: -1.388\n",
      "Episode: 238, total steps: 5458, episode steps: 22, reward: 22.0, loss: -1.361\n",
      "Episode: 239, total steps: 5472, episode steps: 14, reward: 14.0, loss: -1.372\n",
      "Episode: 240, total steps: 5486, episode steps: 14, reward: 14.0, loss: -1.313\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 24.4, Max. Reward: 50.0\n",
      "----------------------------------------\n",
      "Episode: 241, total steps: 5506, episode steps: 20, reward: 20.0, loss: -1.358\n",
      "Episode: 242, total steps: 5520, episode steps: 14, reward: 14.0, loss: -1.416\n",
      "Episode: 243, total steps: 5565, episode steps: 45, reward: 45.0, loss: -1.37\n",
      "Episode: 244, total steps: 5577, episode steps: 12, reward: 12.0, loss: -1.387\n",
      "Episode: 245, total steps: 5608, episode steps: 31, reward: 31.0, loss: -1.388\n",
      "Episode: 246, total steps: 5622, episode steps: 14, reward: 14.0, loss: -1.422\n",
      "Episode: 247, total steps: 5659, episode steps: 37, reward: 37.0, loss: -1.402\n",
      "Episode: 248, total steps: 5679, episode steps: 20, reward: 20.0, loss: -1.425\n",
      "Episode: 249, total steps: 5708, episode steps: 29, reward: 29.0, loss: -1.396\n",
      "Episode: 250, total steps: 5744, episode steps: 36, reward: 36.0, loss: -1.424\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 29.0, Max. Reward: 50.0\n",
      "----------------------------------------\n",
      "Episode: 251, total steps: 5774, episode steps: 30, reward: 30.0, loss: -1.403\n",
      "Episode: 252, total steps: 5794, episode steps: 20, reward: 20.0, loss: -1.399\n",
      "Episode: 253, total steps: 5806, episode steps: 12, reward: 12.0, loss: -1.383\n",
      "Episode: 254, total steps: 5828, episode steps: 22, reward: 22.0, loss: -1.39\n",
      "Episode: 255, total steps: 5844, episode steps: 16, reward: 16.0, loss: -1.379\n",
      "Episode: 256, total steps: 5863, episode steps: 19, reward: 19.0, loss: -1.385\n",
      "Episode: 257, total steps: 5875, episode steps: 12, reward: 12.0, loss: -1.368\n",
      "Episode: 258, total steps: 5907, episode steps: 32, reward: 32.0, loss: -1.403\n",
      "Episode: 259, total steps: 5929, episode steps: 22, reward: 22.0, loss: -1.394\n",
      "Episode: 260, total steps: 5959, episode steps: 30, reward: 30.0, loss: -1.402\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 41.5, Max. Reward: 120.0\n",
      "----------------------------------------\n",
      "Episode: 261, total steps: 5971, episode steps: 12, reward: 12.0, loss: -1.437\n",
      "Episode: 262, total steps: 5991, episode steps: 20, reward: 20.0, loss: -1.429\n",
      "Episode: 263, total steps: 6014, episode steps: 23, reward: 23.0, loss: -1.43\n",
      "Episode: 264, total steps: 6027, episode steps: 13, reward: 13.0, loss: -1.435\n",
      "Episode: 265, total steps: 6052, episode steps: 25, reward: 25.0, loss: -1.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 266, total steps: 6077, episode steps: 25, reward: 25.0, loss: -1.408\n",
      "Episode: 267, total steps: 6092, episode steps: 15, reward: 15.0, loss: -1.415\n",
      "Episode: 268, total steps: 6109, episode steps: 17, reward: 17.0, loss: -1.393\n",
      "Episode: 269, total steps: 6142, episode steps: 33, reward: 33.0, loss: -1.396\n",
      "Episode: 270, total steps: 6162, episode steps: 20, reward: 20.0, loss: -1.429\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 36.4, Max. Reward: 94.0\n",
      "----------------------------------------\n",
      "Episode: 271, total steps: 6176, episode steps: 14, reward: 14.0, loss: -1.377\n",
      "Episode: 272, total steps: 6200, episode steps: 24, reward: 24.0, loss: -1.427\n",
      "Episode: 273, total steps: 6215, episode steps: 15, reward: 15.0, loss: -1.43\n",
      "Episode: 274, total steps: 6249, episode steps: 34, reward: 34.0, loss: -1.388\n",
      "Episode: 275, total steps: 6287, episode steps: 38, reward: 38.0, loss: -1.442\n",
      "Episode: 276, total steps: 6299, episode steps: 12, reward: 12.0, loss: -1.586\n",
      "Episode: 277, total steps: 6321, episode steps: 22, reward: 22.0, loss: -1.513\n",
      "Episode: 278, total steps: 6337, episode steps: 16, reward: 16.0, loss: -1.504\n",
      "Episode: 279, total steps: 6349, episode steps: 12, reward: 12.0, loss: -1.633\n",
      "Episode: 280, total steps: 6363, episode steps: 14, reward: 14.0, loss: -1.556\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 30.9, Max. Reward: 76.0\n",
      "----------------------------------------\n",
      "Episode: 281, total steps: 6386, episode steps: 23, reward: 23.0, loss: -1.546\n",
      "Episode: 282, total steps: 6403, episode steps: 17, reward: 17.0, loss: -1.529\n",
      "Episode: 283, total steps: 6423, episode steps: 20, reward: 20.0, loss: -1.584\n",
      "Episode: 284, total steps: 6439, episode steps: 16, reward: 16.0, loss: -1.574\n",
      "Episode: 285, total steps: 6449, episode steps: 10, reward: 10.0, loss: -1.563\n",
      "Episode: 286, total steps: 6466, episode steps: 17, reward: 17.0, loss: -1.526\n",
      "Episode: 287, total steps: 6483, episode steps: 17, reward: 17.0, loss: -1.498\n",
      "Episode: 288, total steps: 6501, episode steps: 18, reward: 18.0, loss: -1.583\n",
      "Episode: 289, total steps: 6522, episode steps: 21, reward: 21.0, loss: -1.579\n",
      "Episode: 290, total steps: 6548, episode steps: 26, reward: 26.0, loss: -1.565\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 47.6, Max. Reward: 100.0\n",
      "----------------------------------------\n",
      "Episode: 291, total steps: 6572, episode steps: 24, reward: 24.0, loss: -1.576\n",
      "Episode: 292, total steps: 6606, episode steps: 34, reward: 34.0, loss: -1.578\n",
      "Episode: 293, total steps: 6630, episode steps: 24, reward: 24.0, loss: -1.602\n",
      "Episode: 294, total steps: 6665, episode steps: 35, reward: 35.0, loss: -1.568\n",
      "Episode: 295, total steps: 6681, episode steps: 16, reward: 16.0, loss: -1.629\n",
      "Episode: 296, total steps: 6696, episode steps: 15, reward: 15.0, loss: -1.582\n",
      "Episode: 297, total steps: 6713, episode steps: 17, reward: 17.0, loss: -1.581\n",
      "Episode: 298, total steps: 6723, episode steps: 10, reward: 10.0, loss: -1.575\n",
      "Episode: 299, total steps: 6738, episode steps: 15, reward: 15.0, loss: -1.607\n",
      "Episode: 300, total steps: 6763, episode steps: 25, reward: 25.0, loss: -1.565\n",
      "TRAIN STATS: {'critic_1_loss': 0.5483323740959167, 'critic_2_loss': 0.5591420483589172, 'policy_loss': -5.803733253479004, 'ent_loss': 0.0, 'alpha': 0.20000000298023224, 'cnt': 1.0}\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 35.8, Max. Reward: 48.0\n",
      "----------------------------------------\n",
      "Episode: 301, total steps: 6809, episode steps: 46, reward: 46.0, loss: -1.647\n",
      "Episode: 302, total steps: 6831, episode steps: 22, reward: 22.0, loss: -1.603\n",
      "Episode: 303, total steps: 6842, episode steps: 11, reward: 11.0, loss: -1.649\n",
      "Episode: 304, total steps: 6879, episode steps: 37, reward: 37.0, loss: -1.596\n",
      "Episode: 305, total steps: 6907, episode steps: 28, reward: 28.0, loss: -1.619\n",
      "Episode: 306, total steps: 6940, episode steps: 33, reward: 33.0, loss: -1.617\n",
      "Episode: 307, total steps: 6986, episode steps: 46, reward: 46.0, loss: -1.602\n",
      "Episode: 308, total steps: 7010, episode steps: 24, reward: 24.0, loss: -1.598\n",
      "Episode: 309, total steps: 7040, episode steps: 30, reward: 30.0, loss: -1.595\n",
      "Episode: 310, total steps: 7065, episode steps: 25, reward: 25.0, loss: -1.613\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 36.5, Max. Reward: 85.0\n",
      "----------------------------------------\n",
      "Episode: 311, total steps: 7100, episode steps: 35, reward: 35.0, loss: -1.599\n",
      "Episode: 312, total steps: 7126, episode steps: 26, reward: 26.0, loss: -1.564\n",
      "Episode: 313, total steps: 7150, episode steps: 24, reward: 24.0, loss: -1.594\n",
      "Episode: 314, total steps: 7164, episode steps: 14, reward: 14.0, loss: -1.604\n",
      "Episode: 315, total steps: 7191, episode steps: 27, reward: 27.0, loss: -1.61\n",
      "Episode: 316, total steps: 7205, episode steps: 14, reward: 14.0, loss: -1.621\n",
      "Episode: 317, total steps: 7223, episode steps: 18, reward: 18.0, loss: -1.59\n",
      "Episode: 318, total steps: 7238, episode steps: 15, reward: 15.0, loss: -1.581\n",
      "Episode: 319, total steps: 7255, episode steps: 17, reward: 17.0, loss: -1.626\n",
      "Episode: 320, total steps: 7280, episode steps: 25, reward: 25.0, loss: -1.64\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 54.5, Max. Reward: 90.0\n",
      "----------------------------------------\n",
      "Episode: 321, total steps: 7297, episode steps: 17, reward: 17.0, loss: -1.788\n",
      "Episode: 322, total steps: 7313, episode steps: 16, reward: 16.0, loss: -1.72\n",
      "Episode: 323, total steps: 7331, episode steps: 18, reward: 18.0, loss: -1.739\n",
      "Episode: 324, total steps: 7352, episode steps: 21, reward: 21.0, loss: -1.738\n",
      "Episode: 325, total steps: 7362, episode steps: 10, reward: 10.0, loss: -1.744\n",
      "Episode: 326, total steps: 7381, episode steps: 19, reward: 19.0, loss: -1.661\n",
      "Episode: 327, total steps: 7396, episode steps: 15, reward: 15.0, loss: -1.775\n",
      "Episode: 328, total steps: 7411, episode steps: 15, reward: 15.0, loss: -1.748\n",
      "Episode: 329, total steps: 7421, episode steps: 10, reward: 10.0, loss: -1.752\n",
      "Episode: 330, total steps: 7441, episode steps: 20, reward: 20.0, loss: -1.747\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 42.3, Max. Reward: 64.0\n",
      "----------------------------------------\n",
      "Episode: 331, total steps: 7453, episode steps: 12, reward: 12.0, loss: -1.789\n",
      "Episode: 332, total steps: 7465, episode steps: 12, reward: 12.0, loss: -1.789\n",
      "Episode: 333, total steps: 7482, episode steps: 17, reward: 17.0, loss: -1.816\n",
      "Episode: 334, total steps: 7520, episode steps: 38, reward: 38.0, loss: -1.803\n",
      "Episode: 335, total steps: 7538, episode steps: 18, reward: 18.0, loss: -1.74\n",
      "Episode: 336, total steps: 7561, episode steps: 23, reward: 23.0, loss: -1.757\n",
      "Episode: 337, total steps: 7573, episode steps: 12, reward: 12.0, loss: -1.711\n",
      "Episode: 338, total steps: 7591, episode steps: 18, reward: 18.0, loss: -1.744\n",
      "Episode: 339, total steps: 7617, episode steps: 26, reward: 26.0, loss: -1.718\n",
      "Episode: 340, total steps: 7637, episode steps: 20, reward: 20.0, loss: -1.754\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 49.8, Max. Reward: 89.0\n",
      "----------------------------------------\n",
      "Episode: 341, total steps: 7659, episode steps: 22, reward: 22.0, loss: -1.788\n",
      "Episode: 342, total steps: 7669, episode steps: 10, reward: 10.0, loss: -1.746\n",
      "Episode: 343, total steps: 7694, episode steps: 25, reward: 25.0, loss: -1.778\n",
      "Episode: 344, total steps: 7717, episode steps: 23, reward: 23.0, loss: -1.807\n",
      "Episode: 345, total steps: 7755, episode steps: 38, reward: 38.0, loss: -1.791\n",
      "Episode: 346, total steps: 7769, episode steps: 14, reward: 14.0, loss: -1.804\n",
      "Episode: 347, total steps: 7784, episode steps: 15, reward: 15.0, loss: -1.82\n",
      "Episode: 348, total steps: 7805, episode steps: 21, reward: 21.0, loss: -1.758\n",
      "Episode: 349, total steps: 7821, episode steps: 16, reward: 16.0, loss: -1.852\n",
      "Episode: 350, total steps: 7833, episode steps: 12, reward: 12.0, loss: -1.761\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 49.8, Max. Reward: 89.0\n",
      "----------------------------------------\n",
      "Episode: 351, total steps: 7884, episode steps: 51, reward: 51.0, loss: -1.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 352, total steps: 7907, episode steps: 23, reward: 23.0, loss: -1.788\n",
      "Episode: 353, total steps: 7945, episode steps: 38, reward: 38.0, loss: -1.777\n",
      "Episode: 354, total steps: 7979, episode steps: 34, reward: 34.0, loss: -1.812\n",
      "Episode: 355, total steps: 7991, episode steps: 12, reward: 12.0, loss: -1.874\n",
      "Episode: 356, total steps: 8032, episode steps: 41, reward: 41.0, loss: -1.831\n",
      "Episode: 357, total steps: 8058, episode steps: 26, reward: 26.0, loss: -1.848\n",
      "Episode: 358, total steps: 8075, episode steps: 17, reward: 17.0, loss: -1.838\n",
      "Episode: 359, total steps: 8104, episode steps: 29, reward: 29.0, loss: -1.805\n",
      "Episode: 360, total steps: 8153, episode steps: 49, reward: 49.0, loss: -1.825\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 47.2, Max. Reward: 108.0\n",
      "----------------------------------------\n",
      "Episode: 361, total steps: 8170, episode steps: 17, reward: 17.0, loss: -1.829\n",
      "Episode: 362, total steps: 8183, episode steps: 13, reward: 13.0, loss: -1.868\n",
      "Episode: 363, total steps: 8198, episode steps: 15, reward: 15.0, loss: -1.834\n",
      "Episode: 364, total steps: 8248, episode steps: 50, reward: 50.0, loss: -1.829\n",
      "Episode: 365, total steps: 8279, episode steps: 31, reward: 31.0, loss: -1.854\n",
      "Episode: 366, total steps: 8309, episode steps: 30, reward: 30.0, loss: -1.939\n",
      "Episode: 367, total steps: 8321, episode steps: 12, reward: 12.0, loss: -2.006\n",
      "Episode: 368, total steps: 8333, episode steps: 12, reward: 12.0, loss: -2.047\n",
      "Episode: 369, total steps: 8347, episode steps: 14, reward: 14.0, loss: -1.895\n",
      "Episode: 370, total steps: 8358, episode steps: 11, reward: 11.0, loss: -1.971\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 57.1, Max. Reward: 103.0\n",
      "----------------------------------------\n",
      "Episode: 371, total steps: 8371, episode steps: 13, reward: 13.0, loss: -2.031\n",
      "Episode: 372, total steps: 8388, episode steps: 17, reward: 17.0, loss: -1.999\n",
      "Episode: 373, total steps: 8400, episode steps: 12, reward: 12.0, loss: -2.017\n",
      "Episode: 374, total steps: 8419, episode steps: 19, reward: 19.0, loss: -1.968\n",
      "Episode: 375, total steps: 8467, episode steps: 48, reward: 48.0, loss: -2.021\n",
      "Episode: 376, total steps: 8485, episode steps: 18, reward: 18.0, loss: -2.022\n",
      "Episode: 377, total steps: 8517, episode steps: 32, reward: 32.0, loss: -1.975\n",
      "Episode: 378, total steps: 8527, episode steps: 10, reward: 10.0, loss: -1.982\n",
      "Episode: 379, total steps: 8557, episode steps: 30, reward: 30.0, loss: -2.021\n",
      "Episode: 380, total steps: 8578, episode steps: 21, reward: 21.0, loss: -1.992\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 66.2, Max. Reward: 168.0\n",
      "----------------------------------------\n",
      "Episode: 381, total steps: 8593, episode steps: 15, reward: 15.0, loss: -1.962\n",
      "Episode: 382, total steps: 8618, episode steps: 25, reward: 25.0, loss: -1.999\n",
      "Episode: 383, total steps: 8631, episode steps: 13, reward: 13.0, loss: -1.944\n",
      "Episode: 384, total steps: 8650, episode steps: 19, reward: 19.0, loss: -2.067\n",
      "Episode: 385, total steps: 8678, episode steps: 28, reward: 28.0, loss: -2.003\n",
      "Episode: 386, total steps: 8735, episode steps: 57, reward: 57.0, loss: -2.021\n",
      "Episode: 387, total steps: 8751, episode steps: 16, reward: 16.0, loss: -1.933\n",
      "Episode: 388, total steps: 8800, episode steps: 49, reward: 49.0, loss: -2.039\n",
      "Episode: 389, total steps: 8842, episode steps: 42, reward: 42.0, loss: -2.047\n",
      "Episode: 390, total steps: 8860, episode steps: 18, reward: 18.0, loss: -2.002\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 55.4, Max. Reward: 95.0\n",
      "----------------------------------------\n",
      "Episode: 391, total steps: 8907, episode steps: 47, reward: 47.0, loss: -2.048\n",
      "Episode: 392, total steps: 8973, episode steps: 66, reward: 66.0, loss: -2.025\n",
      "Episode: 393, total steps: 8989, episode steps: 16, reward: 16.0, loss: -2.117\n",
      "Episode: 394, total steps: 9002, episode steps: 13, reward: 13.0, loss: -2.092\n",
      "Episode: 395, total steps: 9016, episode steps: 14, reward: 14.0, loss: -2.053\n",
      "Episode: 396, total steps: 9063, episode steps: 47, reward: 47.0, loss: -2.026\n",
      "Episode: 397, total steps: 9096, episode steps: 33, reward: 33.0, loss: -2.02\n",
      "Episode: 398, total steps: 9107, episode steps: 11, reward: 11.0, loss: -2.084\n",
      "Episode: 399, total steps: 9122, episode steps: 15, reward: 15.0, loss: -2.037\n",
      "Episode: 400, total steps: 9147, episode steps: 25, reward: 25.0, loss: -2.057\n",
      "TRAIN STATS: {'critic_1_loss': 0.5605512499809265, 'critic_2_loss': 0.5740299332141876, 'policy_loss': -7.30572904586792, 'ent_loss': 0.0, 'alpha': 0.20000000298023224, 'cnt': 1.0}\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 55.8, Max. Reward: 99.0\n",
      "----------------------------------------\n",
      "Episode: 401, total steps: 9169, episode steps: 22, reward: 22.0, loss: -2.073\n",
      "Episode: 402, total steps: 9194, episode steps: 25, reward: 25.0, loss: -2.098\n",
      "Episode: 403, total steps: 9207, episode steps: 13, reward: 13.0, loss: -2.076\n",
      "Episode: 404, total steps: 9222, episode steps: 15, reward: 15.0, loss: -2.11\n",
      "Episode: 405, total steps: 9241, episode steps: 19, reward: 19.0, loss: -2.064\n",
      "Episode: 406, total steps: 9278, episode steps: 37, reward: 37.0, loss: -2.05\n",
      "Episode: 407, total steps: 9292, episode steps: 14, reward: 14.0, loss: -2.17\n",
      "Episode: 408, total steps: 9305, episode steps: 13, reward: 13.0, loss: -2.247\n",
      "Episode: 409, total steps: 9315, episode steps: 10, reward: 10.0, loss: -2.275\n",
      "Episode: 410, total steps: 9330, episode steps: 15, reward: 15.0, loss: -2.227\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 62.0, Max. Reward: 83.0\n",
      "----------------------------------------\n",
      "Episode: 411, total steps: 9347, episode steps: 17, reward: 17.0, loss: -2.296\n",
      "Episode: 412, total steps: 9366, episode steps: 19, reward: 19.0, loss: -2.206\n",
      "Episode: 413, total steps: 9381, episode steps: 15, reward: 15.0, loss: -2.211\n",
      "Episode: 414, total steps: 9391, episode steps: 10, reward: 10.0, loss: -2.32\n",
      "Episode: 415, total steps: 9408, episode steps: 17, reward: 17.0, loss: -2.231\n",
      "Episode: 416, total steps: 9425, episode steps: 17, reward: 17.0, loss: -2.248\n",
      "Episode: 417, total steps: 9444, episode steps: 19, reward: 19.0, loss: -2.276\n",
      "Episode: 418, total steps: 9462, episode steps: 18, reward: 18.0, loss: -2.22\n",
      "Episode: 419, total steps: 9497, episode steps: 35, reward: 35.0, loss: -2.221\n",
      "Episode: 420, total steps: 9518, episode steps: 21, reward: 21.0, loss: -2.299\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 61.6, Max. Reward: 108.0\n",
      "----------------------------------------\n",
      "Episode: 421, total steps: 9541, episode steps: 23, reward: 23.0, loss: -2.223\n",
      "Episode: 422, total steps: 9557, episode steps: 16, reward: 16.0, loss: -2.262\n",
      "Episode: 423, total steps: 9601, episode steps: 44, reward: 44.0, loss: -2.313\n",
      "Episode: 424, total steps: 9622, episode steps: 21, reward: 21.0, loss: -2.235\n",
      "Episode: 425, total steps: 9654, episode steps: 32, reward: 32.0, loss: -2.267\n",
      "Episode: 426, total steps: 9687, episode steps: 33, reward: 33.0, loss: -2.239\n",
      "Episode: 427, total steps: 9705, episode steps: 18, reward: 18.0, loss: -2.248\n",
      "Episode: 428, total steps: 9718, episode steps: 13, reward: 13.0, loss: -2.234\n",
      "Episode: 429, total steps: 9730, episode steps: 12, reward: 12.0, loss: -2.426\n",
      "Episode: 430, total steps: 9744, episode steps: 14, reward: 14.0, loss: -2.305\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 65.0, Max. Reward: 123.0\n",
      "----------------------------------------\n",
      "Episode: 431, total steps: 9777, episode steps: 33, reward: 33.0, loss: -2.275\n",
      "Episode: 432, total steps: 9788, episode steps: 11, reward: 11.0, loss: -2.266\n",
      "Episode: 433, total steps: 9806, episode steps: 18, reward: 18.0, loss: -2.293\n",
      "Episode: 434, total steps: 9824, episode steps: 18, reward: 18.0, loss: -2.254\n",
      "Episode: 435, total steps: 9854, episode steps: 30, reward: 30.0, loss: -2.255\n",
      "Episode: 436, total steps: 9869, episode steps: 15, reward: 15.0, loss: -2.241\n",
      "Episode: 437, total steps: 9882, episode steps: 13, reward: 13.0, loss: -2.248\n",
      "Episode: 438, total steps: 9894, episode steps: 12, reward: 12.0, loss: -2.378\n",
      "Episode: 439, total steps: 9919, episode steps: 25, reward: 25.0, loss: -2.286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, total steps: 9941, episode steps: 22, reward: 22.0, loss: -2.258\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 70.1, Max. Reward: 176.0\n",
      "----------------------------------------\n",
      "Episode: 441, total steps: 9962, episode steps: 21, reward: 21.0, loss: -2.279\n",
      "Episode: 442, total steps: 9981, episode steps: 19, reward: 19.0, loss: -2.28\n",
      "Episode: 443, total steps: 10063, episode steps: 82, reward: 82.0, loss: -2.321\n",
      "Episode: 444, total steps: 10176, episode steps: 113, reward: 113.0, loss: -2.307\n",
      "Episode: 445, total steps: 10234, episode steps: 58, reward: 58.0, loss: -2.311\n",
      "Episode: 446, total steps: 10293, episode steps: 59, reward: 59.0, loss: -2.367\n",
      "Episode: 447, total steps: 10400, episode steps: 107, reward: 107.0, loss: -2.514\n",
      "Episode: 448, total steps: 10486, episode steps: 86, reward: 86.0, loss: -2.512\n",
      "Episode: 449, total steps: 10605, episode steps: 119, reward: 119.0, loss: -2.538\n",
      "Episode: 450, total steps: 10646, episode steps: 41, reward: 41.0, loss: -2.531\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: 86.1, Max. Reward: 173.0\n",
      "----------------------------------------\n",
      "Episode: 451, total steps: 10702, episode steps: 56, reward: 56.0, loss: -2.51\n",
      "Episode: 452, total steps: 10753, episode steps: 51, reward: 51.0, loss: -2.559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d055530159e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mcritic_1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0m_train_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0m_train_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_1_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cascade_rl/sac.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "last_total_steps = 0\n",
    "updates = 0\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    train_stats = []\n",
    "    while not done:\n",
    "        if WARM_UP > total_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(state)\n",
    "            action = int(action > 0.5)\n",
    "        \n",
    "        if len(memory) > BATCH_SIZE:\n",
    "            critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, BATCH_SIZE, updates)\n",
    "            _train_stats = Dict()\n",
    "            _train_stats.critic_1_loss = critic_1_loss\n",
    "            _train_stats.critic_2_loss = critic_2_loss\n",
    "            _train_stats.policy_loss = policy_loss\n",
    "            _train_stats.ent_loss = ent_loss\n",
    "            _train_stats.alpha = alpha\n",
    "            train_stats.append(_train_stats)\n",
    "            updates += 1\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "        memory.push(state, np.array([action]), reward, next_state, mask)\n",
    "\n",
    "        state = next_state\n",
    "    train_stats = average_stats(train_stats)\n",
    "    loss = get_avg_loss(train_stats)\n",
    "    print(\"Episode: {}, total steps: {}, episode steps: {}, reward: {}, loss: {}\".format(i_episode, total_steps, episode_steps, round(episode_reward, 2), round(loss, 3)))\n",
    "    if updates > 0 and i_episode % PRINT_FREQ == 0:\n",
    "        print('TRAIN STATS: %s' % str(train_stats))\n",
    "    \n",
    "    if i_episode % 10 == 0:\n",
    "        avg_reward = 0.\n",
    "        max_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.select_action(state)\n",
    "                action = int(action > 0.5)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "            max_reward = max(episode_reward, max_reward)\n",
    "        avg_reward /= episodes\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}, Max. Reward: {}\".format(episodes, round(avg_reward, 2), round(max_reward, 2)))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
